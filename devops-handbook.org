* The Three Ways
** The First Way: Flow
** The Second Way: The Principles of Feedback
   - In complex systems, no way to achieve perfection
   - Next best thing is detecting something gone wrong early to avoid a catastrophe
   - Treat failure as learning opportunity, not chance to play blame game
*** Working Safely Within Complex Systems
    - Complex work is managed so that problems in design and operations are revealed
    - Problems are swarmed and solved, resulting in quick construction of new knowledge
    - new local knowledge is exploited globally throughout the organization
    - leaders create other leaders who continually grow these types of capabilities
*** See problems as they occur
    - feedback loops
    - automated build, integration, and test
    - production telemetry to determine operational health
*** Swarm and solve problems to build new knowledge
    - steps
      - if something goes wrong or work is taking too long, alert the team
        leader
      - if the problem isn't resolved within a specified time, have rest of team
        work until problem is resolved
    - necessary because
      - prevents the problem from progressing downstream, where the cost and
        effort to repair it increases exponentially and technical debt is
        allowed to accumulated
      - prevents work center from starting new work, which will likely introduce
        new errors into the system
      - if problem is not addressed, the work center could potentially have the
        same problem in the next operation (e.g., fifty-five seconds later),
        requiring more fixes and work
    - need equivalent to andon cord
      - production incident
      - breaking change in continuous build or test processes
*** keep pushing quality closer to the source
    - examples of ineffective quality controls include:
      - requiring another team to complete tedious, error-prone, and manual
        tasks that could be easily automated and run as needed by the team who
        needs the work performed
      - requiring approvals from busy people who are distant from the work,
        forcing them to make decisions without an adequate knowledge of the work
        or the potential implications, or to merely rubber stamp their approvals
      - creating large volumes of documentation of questionable detail which
        become obsolete shortly after they are written
      - pushing large batches of work to teams and special committees for
        approval and processing and then waiting for responses
    - instead, everyone in value stream should find and fix problems in their
      area of control as part of daily work
    - use peer reviews
    - automate QA and IA as much as possible
    - make quality everyone's responsibility
*** enable optimizing for downstream work centers
    - *according to Lean, our most important customer is our next step downstream*
      - for example, the lab SAs and the CIT/SAT testers
    - operational non-functional requirements (e.g. architecture, performance,
      stability, testability, configurability, and security) are prioritized as
      highly as user features
** The Third Way: the principles of continual learning and experimentation
   - low performing environments discourage making mistakes and learning
   - high performing environments allow workers in the value stream to
     experiment with new improvements, enabled by rigorous standardization of
     work procedures and documentation of the results
   - reserve time for improvement of daily work and learning
   - introduce stress into systems to force continual improvement
   - simulate and inject failures in production services under controlled
     conditions to increase resilience
*** enabling organizational learning and a safety culture
    - when bad things happen, root cause analysis often leads to blame game and
      more top down processes and approvals
    - "Responses to incidents and accidents that are seen as unjust can impede
      safety investigations, promote fear rather than mindfulness in people who
      do safety-critical work, make organizations more bureaucratic rather than
      more careful, and cultivate professional secrecy, evasion, and self-protection"
    - three types of culture:
      - Pathological organizations are characterized by large amounts of fear
        and threat. People often hoard information, withhold it for political
        reasons, or distort it to make themselves look better. Failure is often hidden.
      - Bureaucratic organizations are characterized by rules and processes,
        often to help individual departments maintain their "turf". Failure is
        processed through a system of judgment, resulting in either punishment
        or justice and mercy.
      - Generative organizations are characterized by actively seeking and
        sharing information to better enable the organization to achieve its
        mission. Resposibilities are shared throughout the value stream, and
        failure results in reflection and genuine inquiry.
*** institutionalize the improvement of daily work
    - "even more important than daily work is the improvement of daily work"
*** transform local discoveries into global improvements
    - make blameless post-morten reports searchable by teams trying to solve
      similar problems
    - create shared source code repositories that span the entire organization
*** inject resilience patterns into our daily work
    - antifragility (Nassim Nicholas Taleb)
    - reduce deployment lead times
    - increase test coverage
    - decrease test execution times
    - re-architect if necessary to increase developer productivity or increase reliability
    - game day exercises, where we rehearse large scale failures, such as
      turning off entire data centers
    - inject ever larger scale faults into production environment
*** leaders reinforce a learning culture
    - explicitly state True North goals, such as "sustain zero accidents"
    - strategic goals inform iterative, shorter term goals
    - coaching questions for people conducting experiment
      - what was your last step and what happened?
      - what did you learn?
      - what is your condition now?
      - what is your next target condition?
      - what obstacle are you working on now?
      - what is your next step?
      - what is your expected outcome?
      - when can we check?
* Initiating a DevOps transformation
** Selecting which value streams to start with
   - example: Nordstrom
     - chose projects to improve who were already struggling to meet business goals
     - mobile app
       - initial state
         - could only release updates twice a year
         - customers were frustrated with product
       - improvement
         - first, enable faster on-demand releases
         - dedicated product team able to independently implement, test, and deliver
         - continuous planning -> single prioritized backlog of work
         - interated testing as daily activity
       - results
         - doubled features delivered per month
         - halved number of defects
     - systems supporting in store restaurants
       - business need
         - decrease cost
         - improve quality
       - initial state
         - in 2013, eleven "restaurant re-concepts" and forty-four more in 2014
         - business leaders suggested tripling team but they decided to improve process
       - improvement
         - work intake and deployment process
         - reduce code deployment times by 60%
         - reduced number of production incidents 60% to 90%
** Greenfield vs Brownfield Services
   - brownfield is well suited for devops
   - example: etsy
     - 35 employees and $87 million in revenue
     - barely survived holiday season and transformed all aspects of the organization
     - became highly admired devops organization and set the stage for a
       successful 2015 IPO
** Consider both systems of record and systems of engagement
   - systems of record
     - systems that run our business
     - MRP, HR, financial reporting
     - correctness of transactions/data are paramount
     - slower pace of change
   - systems of engagement
     - customer or employee facing
     - e-commerce, productivity, etc
     - higher pace of change
   - traditional wisdom says to use these categories to have a "fast
     experimental" and "slow safe" lane
   - however, devops has found that faster also can mean safer, because we're
     getting feedback and fixing problems faster
   - also, because a lot of systems are very interdependent, the ability to make
     changes to any system is often limited by the system that is the most
     difficult to safely change (usually a system of record)
   - scott prugh, VP of product development at CSG, says "We've adopted a
     philosophy that rejects bi-modal IT, because every one of our customers
     deserve speed and quality. This means that we need technical excellence,
     whether the team is supporting a 30 year old mainframe application, a Java
     application, or a mobile application"
** Start with the most sympathetic and innovative groups
** Expanding devops across our organization
   - demonstrate early wins and broadcast success by breaking  up large
     improvement goals into small, incremental steps
   - ideal  phases for change agents to build and expand coalition and support:
     - find innovators and early adopters
       - ideally,  respected  with high degree  of influence
     - build  critical mass and silent majority
     - identify the holdouts
       - high profile, influential detractors
       - tackle only after achieving silent majority
* Understanding the work in our value stream, making it visible, and expanding it across the organization
** Identifying the teams supporting our value stream
   - product owner
   - development
   - qa
   - operations
   - infosec
   - release managers
   - technology  executives or value stream manager
** Create a value stream map to see the work
   - document that answers question "how is work performed?"
   - example
     - customer request or business hypothesis coming through product owner
     - development  implements  at some point
     - build integrated, tested, and finally deployed
   - often  value stream has many steps with hundreds of people involved
   - documenting is complex and may require a  multi-day workshop  with all key
     constituents present
   - goal not to document every step, but to sufficiently understand the areas
     in value stream jeopardizing our goals of fast flow, short lead times,
     and reliable customer outcomes
   - focus on:
     - places where work must wait weeks or even months, such as getting
       production-like  environments, change approval processes, or security
       review processes
     - places where significant rework is  generated or received
   - first pass should only have high  level process blocks, maybe five to fiften
   - each process block should include lead time and process time, as well as
     the percent C/A (complete/accurate) as measured by downstream consumers
   - identify which metric should be improved then construct future value map as
     target condition
   - brainstorm hypotheses and  countermeasures to achieve  desired improvement,
     perform experiments  to test hypotheses, and interpret results to determine
     whether the hypotheses were correct
** Creating a dedicated transformation team
   - how to have change while still  maintaining day to day  operation
     - assign members of dedicated transformation  team to  devops
       transformation efforts solely
     - select team members who are generalists with wide array of skills
     - select team members who have longstanding and mutually respectful
       relationships with rest of organization
     - create separte  physical space for dedicated team if  possible
** Agree on a shared goal
   - achievement of the goal  should create obvious value for the organization
   - define a measurable goal with clearly defined deadline, between  six
     months and two years
   - don't have too many of these going on at the same time
   - examples
     - reduce the percentage of the budget spent on product  support  and
       unplanned  work by 50%
     - ensure lead time from code check-in to production release is  one week or
       less for 95% of changes
     - ensure releases can always  be  performed during normal business hours
       with zero downtime
     - integrate all the required information security controls into the
       deployment pipeline to pass all  required compliance  requirements
   - have several  week iterations to progress by steps towards larger goal
** Keep our improvement planning horizons short
   - flexibility  and ability to reprioritize and replan quickly
   - decrease delay between work expended and improvement realized, which
     strengthens feedback  loop
   - faster learning generated from first iteration
   - reduction  in activation energy to get improvements
   - quicker realization of improvements that make meaningful differences in
     daily work
   - less risk that our project is killed before we can generate any
     demonstrable outcomes
** Reserve 20% of cycles for non-functional requirements and reducing technical debt
*** Case Study: Operation InVersion at LinkedIn (2011)
    - initial state
      - monolithic Java application (Leo) that served every page through servlets and
        managed jdbc connections to various oracle databases
      - in 2010, most new development was in new services outside of Leo but Leo
        was only deployed every two weeks
      - Leo often went down in production, difficult to troubleshoot and recover,
        difficult to release new code
      - "when LinkedIn would try to  add a bunch of new things at  once, the site
        would crumble into a broken mess, requiring engineers to work long into
        the night and fix the problems"
    - solution
      - decided to stop work on new features and focus on fixing infrastructure:
        Operation InVersion
      - developed suite of software and tools to help develop code for site
      - engineers could develop service, inspect it with automated tests, and
        launch it into production
      - major upgrades now hapen three times a day
*** Increase the visibility of work
    - need up to date data
    - constantly revise what we measure to make sure it's helping us understand
      progress toward current target conditions
** Use tools to reinforce desired behavior
   - tooling should reinforce that development and operations have shared goals,
     shared backlog of work, in common work system using shared vocabulary so
     that work can be prioritized globally
   - chat rooms reinforce shared goals by allowing fast flow of information
     - have to monitor  so that there  aren't too many interruptions
* How to design our organization and architecture with Conway's law in mind
  - Conway's law "organizations which design systems are constrained to produce
    designs which are copies of the communication structures  of these
    organizations...The larger an organization is, the less  flexibility it has
    and the more pronounced the phenomenon"
  - example:  Etsy Sprouter
    - problem
      - sprouter hid the database from the application
      - required devs to ask dbas for any new stored procedures, had to wade
        through bereaucracy
      - conway's law, sprouter arose because two separate teams, dev and dbas
      - but sprouter, which was intended to isolate the teams from each other,
        became another layer that had to be maintained anytime anything in either
        team changed
    - solution
      - moved all business logic from database into application, removing need
        for sprouter
      - by eliminating sprouter, they eliminated cross team dependencies, number
        of handoffs, and increased speed/success of production deployments
** Organizational Archetypes
   - functional
     - optimize for expertise, division of labor, or reducing cost
     - centralize expertise
     - tall hierarchical structures
   - matrix
     - attempt to combine functional and market
     - complicated organizational structure, such as people reporting to two
       different managers
   - market
     - optimize for responding quickly to customer needs
     - flat structure, composed of multiple cross functional disciplines, often
       leading to potential redundancies across the company
     - devops organizations tend to use this structure
** Problems often caused by overly functional orientation ("optmizing for cost")
   - slow lead time because of many handoffs between functional groups (devs,
     dbas, SAs, etc)
   - person doing the work has little awareness of how it contributes to the
     value stream ("Someone told me to configure this server"), creating a
     creativity and motivation vacuum
   - problem increased when each operations functional group has to serve
     multiple development teams who all compete for their cycles
   - development teams have to escalate issues to managers who can globally
     prioritize and this cascades down to the functional groups
** Enable market-oriented teams ("optimizing for speed")
   - market oriented teams responsible for development, testing, securing,
     deploying, and supporting service in production
   - enables each team to deliver independently from other teams
** Making functional orientation work
   - work is prioritized transparently and sufficient slack in system to allow
     high-priority work to be completed quickly
   - enabled by automated self-service platforms
   - Mike Rother wrote in Toyota Kata "As tempting as it seems, one cannot
     reorganize your way to continuous improvement and adaptiveness. What is
     decisive is not the form of the organization, but how people act and react.
     The roots of Toyota's success lie not in its organizational structures, but
     in developing capability and habits in its people. It surprises many, in
     fact, to find that Toyota is largely organized in a traditional,
     functional-department style"
** Testing, Operations, and Security as everyone's job, every day
   - facebook was struggling with issues in production
   - one solution was to have every engineer, engineering manager, and architect
     rotate on-call duty for the services they built
** Enable every team member to be a generalist
   - if everyone is only a specialist, too many handoffs
   - specialize but also generalize
   - invest in the people we have by encouraging them to learn and grow
   - yes they may be more expensive than specialists but they are also much more productive
** Fund not projects, but services and products
   - don't move developers to another project after initial development is done
   - have developers stick around on same project for the brownfield phase so
     they learn to live with the errors they've made and how to make the product
     more resilient/useful in production
** Design team boundaries in accordance with Conway's law
   - separate testers and devs can create communication issues
** Create loosely coupled architectures to enable developer productivity and safety
   - services that can be updated independently of each other
   - no shared databases between services
   - bounded contexts between services enforce encapsulation - devs should only
     need to know other services' api
** Keep team sizes small (the "two-pizza team" rule)
   - usually about five to ten people
   - important effects:
     - clear, shared understanding of the system
     - limit growth rate of product or service. Helps ensure that people
       continue to understand service
     - decentralizes power and enables autonomy. Team lead works with executive
       team to determine key metric by which team can measure its progress
     - small teams allows more leadership experience without catastrophic results
*** Case study: API enablement at Target (2015)
    - problem
      - took ten different teams to provision server
      - when things broke, stopped making changes out of fear
      - core data locked up in legacy mainframes
      - multiple sources of truth, with different teams, data structures, priorities
      - long amounts of time spent on integration between various systems, with
        lots of manual testing
    - solution
      - API Enablement Team
      - internal team, no contractors, to ensure low lag time and top
        engineering skill
      - took over ops responsibilities
      - started using cassandra and kafka for scaling (asked permission, was
        refused, and did it anyway because they needed it)
      - in following two years, enabled fifty-three new business capabilities
      - eighty deployments per week
      - digital sales increased 42% during 2014
      - 280k in-store pickup orders in 2015 black friday
* How to get great outcomes by integrating operations into the daily work of development
** Create shared services to increase developer productivity
   - most of these services should be self service, without requiring submitting
     a ticket, so that operations doesn't become a bottleneck
   - "without these self-service operations platforms, the cloud is just
     Expensive Hosting 2.0"
   - examples
     - shared version control repository with pre-blessed security libraries
     - deployment pipeline that automatically runs code quality and security
       scanning tools
     - deploys into known, good environments that already have production
       monitoring tools installed
   - platform team can educate development teams and spread best practices and
     tools across teams so that it's easier for developers to move between teams
     without always having to relearn a new set of tools
** Embed ops engineers into our service teams
   - product teams often have the budget to fund hiring these ops engineers
   - these ops engineers are driven by team priorities, not overall priorities
     and internal ops concerns
   - efficient way to cross-train operations knowledge and expertise into
     service team
** Assign an ops liason to each service team
   - liason responsible for understanding:
     - what is the new product functionality and why we're building it
     - how it works as it pertains to operability, scalability, and observability
     - how to monitor and collect metrics to ensure the progress, success, or
       failure of the functionality
     - any departures from previous architectures and patterns, and the
       justifications for them
     - any extra needs for infrastructure and how usage will affect
       infrastructure capacity
     - feature launch plans
** Integrate ops into dev rituals
** Invite ops to our dev standups
** Invite ops to our dev retrospectives
** Make relevant ops work visible on shared kanban boards
   - put operations work relevant to product delivery on kanban board
* Part III: The First Way - The technical practices of flow
** Create the foundations of our deployment pipeline
   - production like environments at every stage of value stream
   - created in automated, on demand way
*** Enable on demand creation of dev, test, and production environments
    - use automation for any or all of the following
      - copying a virtualized environment (eg vmware image, running a vagrant
        script, booting an amazon machine image file in ec2)
      - building an automated environment creation process that starts from "bare
        metal" (eg PXE install from a baseline image)
      - using "infrastructure as code" configuration management tools (eg puppet,
        chef, ansible salt, cfeengine, etc)
      - using automated operating system configuration tools (eg solaris
        jumpstart, red hat kickstart, debian preseed)
      - assembling an environment from a set of virtual images or containers (eg
        vagrant, docker)
      - spinning up a new environment in a public cloud (eg amazon web services,
        google app engine, microsoft azure), private cloud, or other paas (platform
        as a service, such as openstack or cloud foundry, etc)
    - providing developers environment they fully control allows them to test
      changes to environment, get feedback quickly, etc
*** Create our single repository of truth for the entire system
    - put everything, including QA, operations, infosec, etc into revision control
    - check in the following assets to our shared version control repository
      - all application code and dependencies (eg libraries, static content, etc)
      - any script used to create database schemas, application reference data, etc
      - all environment creation tools and artifacts (eg vmware or ami images,
        puppet or chef recipes, etc)
      - any file used to create containers (eg docker or rocket definition or
        composition files)
      - all supporting automated tests and any manual test scripts
      - any script that supports code packaging, deployment, database migration,
        and environment provisioning
      - all project artifacts (eg requirements docs, deployment procedures,
        release notes, etc)
      - all cloud configuration files (eg aws cloudformation templates, microsoft
        azure stack dsc files, openstack heat)
      - any other script or configuration information required to create
        infrastructure that supports multiple services (eg enterprise service
        buses, database management systems, dns zone files, configuration rules
        for firewalls, and other networking devices)
    - must be able to recreate pre-production and build processes as well as
      production environments (ie tools, compilers, testing)
    - whether ops used version control higher predictor for it performance and
      organizational performance than whether dev used version control
      - why? orders of magnitude more configurable settings in environment than code
*** Make infrastructure easier to rebuild than to repair
    - immutable infrastructure
    - prevent uncontrolled configuration variances
      - disable remote logins to production servers
      - routinely kill or replace production instances
*** Modify our definition of development "done" to include running in production-like environments
** Enable Fast and Reliable Automated Testing
*** Continuously build, test, and integrate our code and environments
    - our build and test processes can run all the time, independent of the work
      habits of individual engineers
    - a segregated build and test process ensures that we understand all the
      dependencies required to build, package, run, and test our code (ie
      removing the "it worked on the developer's laptop, but it broke in
      production" problem)
    - we can package our application to enable the repeatable installation of
      code and configurations into our environment (eg on linux rpm, yum, npm;
      on windows, oneget; alternatively framework-specific packaging systems can
      be used, such as ear and war files for java, gems for ruby, etc)
    - instead of putting our code in packages, we may choose to package our
      applications into deployable containers (eg docker, rkt, lxd, amis)
    - environments can be made more production-like in a way that is consistent
      and repeatable (eg compilers are removed from the environment, debugging
      flags are turned off, etc)
    - pipeline stores history, including information about which tests were
      performed on which build, which builds have been deployed to which
      environment, and what the test results were
    - continuous integration practices
      - comprehensive and reliable set of automated tests that validate we are
        in a deployable state
      - a culture that "stops the entire production line" when our validation
        tests fail
      - developers working in small batches on trunk rather than long-lived
        feature branches
*** Build a fast and reliable automated validation test suite
    - nightly build not fast enough feedback
      - takes longer to find out what change broke the build
      - issue might have been caused by a test environment issue, which may only
        reappear the next night
      - people are checking in more changes with more potential breakers while
        we're fixing the current stuff
      - automated tests
        - unit tests: does what developer expects
        - acceptance tests: does what customer expects
        - integration tests: our application correctly interacts with other
          production applications and services. These tests are brittle and long
          running.
*** Catch errors as early in our automated testing as possible
    - find errors as early as possible
    - errors should be found with the fastest category of testing possible
    - finding/fixing integration test failures is painful, especially since the
      feedback loop can be hours
    - whenever we find an error with an acceptance/integration test, we should
      create a unit test that could find error faster, earlier, and cheaper
    - ideal testing automation pyramid
      - starting at bottom
      - automated unit tests
      - automated component tests
      - automated integration tests
      - automated api tests
      - automated gui tests
      - manual session based testing
    - non-ideal testing automation inverted pyramid
      - starting at bottom
      - manual test
      - automated gui test
      - integration test
      - unit test
    - if unit or acceptance tests are too difficult and expensive to write and
      maintain, likely we have architecture too tightly-coupled, where strong
      separation between module boundaries no longer exist. Need to create more
      loosely-coupled system so modules can be independently tested without
      integration environments
    - acceptance test suites for even most complex applications that run in
      minutes are possible
*** Ensure tests run quickly (in parallel, if necessary)
    - design tests to run in parallel, potentially across many different servers
    - run different categories of tests in parallel
    - for example, when build passes acceptance tests, may run our performance
      testing in parallel with security testing
    - may or may not allow manual exploratory testing until build has passed all
      automated tests - which enables faster feedback, but may also allow manual
      testing on builds that will eventually fail
*** Write our automated tests before we write code
    - ensure tests fail. Check in
    - ensure tests pass. Check in
    - refactor both new and old code to make it well structured. Ensure tests
      pass. Check in again
*** Automate as many of our manual tests as possible
    - by automating what tests we can, we enable all testers to work on
      high-value activities that cannot be automated, such as exploratory
      testing or improving the test process itself
    - we do not want automated tests that are unreliable or generate false positives
    - unreliable tests create significant problems
      - waste valuable time, increase overall effort of running and interpreting
        test results, and often lead to stressed developers ignoring test
        results entirely or turning off automated tests
      - to mitigate, a small number of reliable, automated tests almost always
        preferable over a large number of manual or unreliable automated tests
      - focus on automating only tests that genuinely validate business goals we
        are trying to achieve
*** Integrate performance testing into our test suite
    - detect conditions like
      - database query times grow non-linearly
      - code change causes number of database calls, storage use, or network
        traffic to increase ten-fold
      - code change causes number of database calls, storage use, network
        traffic to increase ten-fold
    - creating performance testing environment can easily be more complex than
      creating the production environment for application itself
    - may build performance testing environment at start of project and ensure
      that we dedicate whatever resources are required to build it early and correctly
    - should log performance results and evaluate each performance run against
      previous results
*** Integrate non-functional requirements testing into our test suite
    - availability, scalability, capacity, security, so forth
    - want to enforce consistency and correctness of following
      - supporting application, databases, libraries, etc
      - language interpreters, compilers, etc
      - operating systems (eg audit logging enabled, etc)
      - all dependencies
    - infrastructure as code configuration, can use same testing frameworks we
      use to test our code to also test our environments are configured and
      operating correctly (eg encoding environment tests into cucumber or
      gherkin tests)
    - should run tools that analyze code that constructs environments
      (foodcritic for chef, puppet-lint for puppet)
    - run any security hardening checks as part of automated tests
    - must create andon cord so that when someone breaks deployment pipeline, we
      take all necessary steps to get back into green build state
*** Pull our andon cord when deployment pipeline breaks
    - when someone introduces change that causes our build to fail, no new work
      is allowed to enter system until problem is fixed
    - if someone needs help to resolve problem, they can bring in whatever help
      they need
    - when pipeline broken, notify entire team, so anyone can fix or roll-back commit
    - may even configure version control system to prevent further code commits
      until first stage (builds and unit tests) of the pipeline is green
    - to increase visibility of automated test failures, we should create highly
      visible indicators so that entire team can see when build or tests are failing
    - this step more challenging than creating builds and test servers, since
      this requires changing human behavior and incentives
*** Why we need to pull the andon cord
    - if we don't pull the cord
      - someone checks in breaking code but no one fixes it
      - someone checks in another change onto broken build, but no one sees the
        failing test results which are hidden by first failures
      - our existing tests don't run reliably, so we are very unlikely to build
        new tests
    - when these things happen, deployments to any environment become as
      unreliable as when we had no automated tests or using waterfall method,
      where majority of our problems are discovered in production
*** Conclusion
    - set the stage for implementing continuous integration, which allows many
      small teams to independently and safely develop, test, and deploy code
      into production
** Enable and practice continuous integration
   - long running branches makes integration much harder
   - automated tests allow continuous integration without constantly breaking things
*** Small batch development and what happens when we commit code to trunk infrequently
    - people more reluctant to refactor because creates even more difficult merges
*** Adopt trunk-based development practices
    - gated commits - deployment pipeline confirms that submitted change will
      successfully merge, build as expected, and pass all automated tests before
      actually being merged into trunk. If not, developer will be notified
** Automate and enable low-risk releases
*** Automate our deployment process
    - document existing deployment to production process, such as:
      - packaging code in ways suitable for deployment
      - creating pre-configured virtual machine images or containers
      - automating the deployment and configuration of middleware
      - copying packages or files onto production servers
      - restarting servers, applications, or services
      - generating configuration files from templates
      - running automated smoke tests to make sure the system is working and
        correctly configured
      - running testing procedures
      - scripting and automating database migrations
    - reduce lead times and handoffs to reduce errors and loss of knowledge
    - development must work closely with operations to ensure all tools and
      processes we co-create can be used downstream
    - requirements for deployment pipeline include:
      - deploying the same way to every environment
      - smoke testing our deployments
      - ensure we maintain consistent environments
*** Enable automated self-service deployments
    - "as a developer, there has never been a more satisfying point in my career
      than when I wrote the code, when I pushed the button to deploy it, when I
      could see the production metrics confirm that it actually worked in
      production, and when I could fix it myself if it didn't"
    - not significant difference in reliability if dev or operations actually do deploys
    - to enable fast flow, want code promotion process that can be performed by
      either dev or operations, ideally without manual steps or handoffs. This
      affects following steps:
      - build: must create packages from version control that can be deployed to
        any environment, including production
      - test: anyone should be able to run any or all of our automated test
        suite on their workstation or on test systems
      - deploy: anybody should be able to deploy packages to any environment
        where they have access, executed by running scripts that are checked
        into version control
*** Integrate code deployment into deployment pipeline
    - our deployment automation must provide following capabilities:
      - ensure packages created during continuous integration are suitable for
        deployment into production
      - show readiness of production environments at a glance
      - provide push-button, self-service method for any suitable version of
        packaged code to be deployed into production
      - record automatically, for auditing and compliance purposes, which
        commands were run on which machines when, who authorized it, and what
        output was
      - run smoke test to ensure system is operating correctly and configuratin
        settings, including items such as database connection strings, are correct
      - provide fast feedback for deployer so they can quickly determine whether
        it was successful
    - case study: etsy - self-service developer deployment
      - run unit tests in parallel on multiple jenkins instances
      - run smoke tests and gui driven tests
*** Decouple deployments from releases
    - deployment is installation of specified version of software into given environment
    - release is when we make a feature or set of features available to all our
      customers or segment of customers. Our code and environments should be
      architected in such a way that release of functionality does not require
      changing our application code
    - as we deploy faster, how quickly we expose new functionality becomes
      business and marketing decision, not technical decision
    - two broad categories of release patterns
      - *environment based release patterns*: we have two or more environments
        we deploy into, but only one receiving live traffic (via load
        balancers). New code deployed into non-live and release is performed
        moving traffic to environment.
        - blue-green deployments
        - canary releases
        - cluster immune systems
      - *application-based release patterns*: modify application so we
        selectively release and expose specific application functionality by
        small configuration changes. Feature flags that progressively expose new
        functionality in production to devs, all internal employees, 1% of
        customers, or everyone
        - enables dark launching, where we stage all functionality to be
          launched in production and test it with production traffic
**** environment based release patterns
***** blue green
      - two production systems, blue and green
      - at any time, only one serving customer traffic
      - deploy to inactive system, test it out, then redirect traffic
      - rollback is performed by redirecting to the old system
****** Dealing with database changes
       - two approaches
         - create two databases
           - blue (old) and green (new)
           - during release, put blue database into read-only mode, perform
             backup, restore onto green, and finally switch traffic
           - problem is if we need to roll back to blue version, we can lose
             transactions if we don't manually migrate from green
         - decouple database changes from application changes
           - release of database changes separate from release of application changes
           - make only additive changes to database, never mutating existing
             database objects
           - no assumptions in application about which database version will be
             in production
****** Case study: Dixons retail - blue-green deployment for point-of-sale system
       - typically upgrading POS systems are big bang waterfall
       - weeks before planned POS upgrade, they started sending out new versions
         of client POS software installers to retail stores over slow network
         links, deploying new software in inactive state
       - when all clients staged (upgraded client and server had tested together
         succesfully, and new client software had been deployed to all clients),
         store managers were empowered to decide when to release new version
       - depending on business needs, some managers wanted to use new features
         immediately while others wanted to wait
