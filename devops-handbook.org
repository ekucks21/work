* The Three Ways
** The First Way: Flow
** The Second Way: The Principles of Feedback
   - In complex systems, no way to achieve perfection
   - Next best thing is detecting something gone wrong early to avoid a catastrophe
   - Treat failure as learning opportunity, not chance to play blame game
*** Working Safely Within Complex Systems
    - Complex work is managed so that problems in design and operations are revealed
    - Problems are swarmed and solved, resulting in quick construction of new knowledge
    - new local knowledge is exploited globally throughout the organization
    - leaders create other leaders who continually grow these types of capabilities
*** See problems as they occur
    - feedback loops
    - automated build, integration, and test
    - production telemetry to determine operational health
*** Swarm and solve problems to build new knowledge
    - steps
      - if something goes wrong or work is taking too long, alert the team
        leader
      - if the problem isn't resolved within a specified time, have rest of team
        work until problem is resolved
    - necessary because
      - prevents the problem from progressing downstream, where the cost and
        effort to repair it increases exponentially and technical debt is
        allowed to accumulated
      - prevents work center from starting new work, which will likely introduce
        new errors into the system
      - if problem is not addressed, the work center could potentially have the
        same problem in the next operation (e.g., fifty-five seconds later),
        requiring more fixes and work
    - need equivalent to andon cord
      - production incident
      - breaking change in continuous build or test processes
*** keep pushing quality closer to the source
    - examples of ineffective quality controls include:
      - requiring another team to complete tedious, error-prone, and manual
        tasks that could be easily automated and run as needed by the team who
        needs the work performed
      - requiring approvals from busy people who are distant from the work,
        forcing them to make decisions without an adequate knowledge of the work
        or the potential implications, or to merely rubber stamp their approvals
      - creating large volumes of documentation of questionable detail which
        become obsolete shortly after they are written
      - pushing large batches of work to teams and special committees for
        approval and processing and then waiting for responses
    - instead, everyone in value stream should find and fix problems in their
      area of control as part of daily work
    - use peer reviews
    - automate QA and IA as much as possible
    - make quality everyone's responsibility
*** enable optimizing for downstream work centers
    - *according to Lean, our most important customer is our next step downstream*
      - for example, the lab SAs and the CIT/SAT testers
    - operational non-functional requirements (e.g. architecture, performance,
      stability, testability, configurability, and security) are prioritized as
      highly as user features
** The Third Way: the principles of continual learning and experimentation
   - low performing environments discourage making mistakes and learning
   - high performing environments allow workers in the value stream to
     experiment with new improvements, enabled by rigorous standardization of
     work procedures and documentation of the results
   - reserve time for improvement of daily work and learning
   - introduce stress into systems to force continual improvement
   - simulate and inject failures in production services under controlled
     conditions to increase resilience
*** enabling organizational learning and a safety culture
    - when bad things happen, root cause analysis often leads to blame game and
      more top down processes and approvals
    - "Responses to incidents and accidents that are seen as unjust can impede
      safety investigations, promote fear rather than mindfulness in people who
      do safety-critical work, make organizations more bureaucratic rather than
      more careful, and cultivate professional secrecy, evasion, and self-protection"
    - three types of culture:
      - Pathological organizations are characterized by large amounts of fear
        and threat. People often hoard information, withhold it for political
        reasons, or distort it to make themselves look better. Failure is often hidden.
      - Bureaucratic organizations are characterized by rules and processes,
        often to help individual departments maintain their "turf". Failure is
        processed through a system of judgment, resulting in either punishment
        or justice and mercy.
      - Generative organizations are characterized by actively seeking and
        sharing information to better enable the organization to achieve its
        mission. Resposibilities are shared throughout the value stream, and
        failure results in reflection and genuine inquiry.
*** institutionalize the improvement of daily work
    - "even more important than daily work is the improvement of daily work"
*** transform local discoveries into global improvements
    - make blameless post-morten reports searchable by teams trying to solve
      similar problems
    - create shared source code repositories that span the entire organization
*** inject resilience patterns into our daily work
    - antifragility (Nassim Nicholas Taleb)
    - reduce deployment lead times
    - increase test coverage
    - decrease test execution times
    - re-architect if necessary to increase developer productivity or increase reliability
    - game day exercises, where we rehearse large scale failures, such as
      turning off entire data centers
    - inject ever larger scale faults into production environment
*** leaders reinforce a learning culture
    - explicitly state True North goals, such as "sustain zero accidents"
    - strategic goals inform iterative, shorter term goals
    - coaching questions for people conducting experiment
      - what was your last step and what happened?
      - what did you learn?
      - what is your condition now?
      - what is your next target condition?
      - what obstacle are you working on now?
      - what is your next step?
      - what is your expected outcome?
      - when can we check?
* Initiating a DevOps transformation
** Selecting which value streams to start with
   - example: Nordstrom
     - chose projects to improve who were already struggling to meet business goals
     - mobile app
       - initial state
         - could only release updates twice a year
         - customers were frustrated with product
       - improvement
         - first, enable faster on-demand releases
         - dedicated product team able to independently implement, test, and deliver
         - continuous planning -> single prioritized backlog of work
         - integrated testing as daily activity
       - results
         - doubled features delivered per month
         - halved number of defects
     - systems supporting in store restaurants
       - business need
         - decrease cost
         - improve quality
       - initial state
         - in 2013, eleven "restaurant re-concepts" and forty-four more in 2014
         - business leaders suggested tripling team but they decided to improve process
       - improvement
         - work intake and deployment process
         - reduce code deployment times by 60%
         - reduced number of production incidents 60% to 90%
** Greenfield vs Brownfield Services
   - brownfield is well suited for devops
   - example: etsy
     - 35 employees and $87 million in revenue
     - barely survived holiday season and transformed all aspects of the organization
     - became highly admired devops organization and set the stage for a
       successful 2015 IPO
** Consider both systems of record and systems of engagement
   - systems of record
     - systems that run our business
     - MRP, HR, financial reporting
     - correctness of transactions/data are paramount
     - slower pace of change
   - systems of engagement
     - customer or employee facing
     - e-commerce, productivity, etc
     - higher pace of change
   - traditional wisdom says to use these categories to have a "fast
     experimental" and "slow safe" lane
   - however, devops has found that faster also can mean safer, because we're
     getting feedback and fixing problems faster
   - also, because a lot of systems are very interdependent, the ability to make
     changes to any system is often limited by the system that is the most
     difficult to safely change (usually a system of record)
   - scott prugh, VP of product development at CSG, says "We've adopted a
     philosophy that rejects bi-modal IT, because every one of our customers
     deserve speed and quality. This means that we need technical excellence,
     whether the team is supporting a 30 year old mainframe application, a Java
     application, or a mobile application"
** Start with the most sympathetic and innovative groups
** Expanding devops across our organization
   - demonstrate early wins and broadcast success by breaking  up large
     improvement goals into small, incremental steps
   - ideal  phases for change agents to build and expand coalition and support:
     - find innovators and early adopters
       - ideally,  respected  with high degree  of influence
     - build  critical mass and silent majority
     - identify the holdouts
       - high profile, influential detractors
       - tackle only after achieving silent majority
* Understanding the work in our value stream, making it visible, and expanding it across the organization
** Identifying the teams supporting our value stream
   - product owner
   - development
   - qa
   - operations
   - infosec
   - release managers
   - technology  executives or value stream manager
** Create a value stream map to see the work
   - document that answers question "how is work performed?"
   - example
     - customer request or business hypothesis coming through product owner
     - development  implements  at some point
     - build integrated, tested, and finally deployed
   - often  value stream has many steps with hundreds of people involved
   - documenting is complex and may require a  multi-day workshop  with all key
     constituents present
   - goal not to document every step, but to sufficiently understand the areas
     in value stream jeopardizing our goals of fast flow, short lead times,
     and reliable customer outcomes
   - focus on:
     - places where work must wait weeks or even months, such as getting
       production-like  environments, change approval processes, or security
       review processes
     - places where significant rework is  generated or received
   - first pass should only have high  level process blocks, maybe five to fiften
   - each process block should include lead time and process time, as well as
     the percent C/A (complete/accurate) as measured by downstream consumers
   - identify which metric should be improved then construct future value map as
     target condition
   - brainstorm hypotheses and  countermeasures to achieve  desired improvement,
     perform experiments  to test hypotheses, and interpret results to determine
     whether the hypotheses were correct
** Creating a dedicated transformation team
   - how to have change while still  maintaining day to day  operation
     - assign members of dedicated transformation  team to  devops
       transformation efforts solely
     - select team members who are generalists with wide array of skills
     - select team members who have longstanding and mutually respectful
       relationships with rest of organization
     - create separte  physical space for dedicated team if  possible
** Agree on a shared goal
   - achievement of the goal  should create obvious value for the organization
   - define a measurable goal with clearly defined deadline, between  six
     months and two years
   - don't have too many of these going on at the same time
   - examples
     - reduce the percentage of the budget spent on product  support  and
       unplanned  work by 50%
     - ensure lead time from code check-in to production release is  one week or
       less for 95% of changes
     - ensure releases can always  be  performed during normal business hours
       with zero downtime
     - integrate all the required information security controls into the
       deployment pipeline to pass all  required compliance  requirements
   - have several  week iterations to progress by steps towards larger goal
** Keep our improvement planning horizons short
   - flexibility  and ability to reprioritize and replan quickly
   - decrease delay between work expended and improvement realized, which
     strengthens feedback  loop
   - faster learning generated from first iteration
   - reduction  in activation energy to get improvements
   - quicker realization of improvements that make meaningful differences in
     daily work
   - less risk that our project is killed before we can generate any
     demonstrable outcomes
** Reserve 20% of cycles for non-functional requirements and reducing technical debt
*** Case Study: Operation InVersion at LinkedIn (2011)
    - initial state
      - monolithic Java application (Leo) that served every page through servlets and
        managed jdbc connections to various oracle databases
      - in 2010, most new development was in new services outside of Leo but Leo
        was only deployed every two weeks
      - Leo often went down in production, difficult to troubleshoot and recover,
        difficult to release new code
      - "when LinkedIn would try to  add a bunch of new things at  once, the site
        would crumble into a broken mess, requiring engineers to work long into
        the night and fix the problems"
    - solution
      - decided to stop work on new features and focus on fixing infrastructure:
        Operation InVersion
      - developed suite of software and tools to help develop code for site
      - engineers could develop service, inspect it with automated tests, and
        launch it into production
      - major upgrades now hapen three times a day
*** Increase the visibility of work
    - need up to date data
    - constantly revise what we measure to make sure it's helping us understand
      progress toward current target conditions
** Use tools to reinforce desired behavior
   - tooling should reinforce that development and operations have shared goals,
     shared backlog of work, in common work system using shared vocabulary so
     that work can be prioritized globally
   - chat rooms reinforce shared goals by allowing fast flow of information
     - have to monitor  so that there  aren't too many interruptions
* How to design our organization and architecture with Conway's law in mind
  - Conway's law "organizations which design systems are constrained to produce
    designs which are copies of the communication structures  of these
    organizations...The larger an organization is, the less  flexibility it has
    and the more pronounced the phenomenon"
  - example:  Etsy Sprouter
    - problem
      - sprouter hid the database from the application
      - required devs to ask dbas for any new stored procedures, had to wade
        through bereaucracy
      - conway's law, sprouter arose because two separate teams, dev and dbas
      - but sprouter, which was intended to isolate the teams from each other,
        became another layer that had to be maintained anytime anything in either
        team changed
    - solution
      - moved all business logic from database into application, removing need
        for sprouter
      - by eliminating sprouter, they eliminated cross team dependencies, number
        of handoffs, and increased speed/success of production deployments
** Organizational Archetypes
   - functional
     - optimize for expertise, division of labor, or reducing cost
     - centralize expertise
     - tall hierarchical structures
   - matrix
     - attempt to combine functional and market
     - complicated organizational structure, such as people reporting to two
       different managers
   - market
     - optimize for responding quickly to customer needs
     - flat structure, composed of multiple cross functional disciplines, often
       leading to potential redundancies across the company
     - devops organizations tend to use this structure
** Problems often caused by overly functional orientation ("optmizing for cost")
   - slow lead time because of many handoffs between functional groups (devs,
     dbas, SAs, etc)
   - person doing the work has little awareness of how it contributes to the
     value stream ("Someone told me to configure this server"), creating a
     creativity and motivation vacuum
   - problem increased when each operations functional group has to serve
     multiple development teams who all compete for their cycles
   - development teams have to escalate issues to managers who can globally
     prioritize and this cascades down to the functional groups
** Enable market-oriented teams ("optimizing for speed")
   - market oriented teams responsible for development, testing, securing,
     deploying, and supporting service in production
   - enables each team to deliver independently from other teams
** Making functional orientation work
   - work is prioritized transparently and sufficient slack in system to allow
     high-priority work to be completed quickly
   - enabled by automated self-service platforms
   - Mike Rother wrote in Toyota Kata "As tempting as it seems, one cannot
     reorganize your way to continuous improvement and adaptiveness. What is
     decisive is not the form of the organization, but how people act and react.
     The roots of Toyota's success lie not in its organizational structures, but
     in developing capability and habits in its people. It surprises many, in
     fact, to find that Toyota is largely organized in a traditional,
     functional-department style"
** Testing, Operations, and Security as everyone's job, every day
   - facebook was struggling with issues in production
   - one solution was to have every engineer, engineering manager, and architect
     rotate on-call duty for the services they built
** Enable every team member to be a generalist
   - if everyone is only a specialist, too many handoffs
   - specialize but also generalize
   - invest in the people we have by encouraging them to learn and grow
   - yes they may be more expensive than specialists but they are also much more productive
** Fund not projects, but services and products
   - don't move developers to another project after initial development is done
   - have developers stick around on same project for the brownfield phase so
     they learn to live with the errors they've made and how to make the product
     more resilient/useful in production
** Design team boundaries in accordance with Conway's law
   - separate testers and devs can create communication issues
** Create loosely coupled architectures to enable developer productivity and safety
   - services that can be updated independently of each other
   - no shared databases between services
   - bounded contexts between services enforce encapsulation - devs should only
     need to know other services' api
** Keep team sizes small (the "two-pizza team" rule)
   - usually about five to ten people
   - important effects:
     - clear, shared understanding of the system
     - limit growth rate of product or service. Helps ensure that people
       continue to understand service
     - decentralizes power and enables autonomy. Team lead works with executive
       team to determine key metric by which team can measure its progress
     - small teams allows more leadership experience without catastrophic results
*** Case study: API enablement at Target (2015)
    - problem
      - took ten different teams to provision server
      - when things broke, stopped making changes out of fear
      - core data locked up in legacy mainframes
      - multiple sources of truth, with different teams, data structures, priorities
      - long amounts of time spent on integration between various systems, with
        lots of manual testing
    - solution
      - API Enablement Team
      - internal team, no contractors, to ensure low lag time and top
        engineering skill
      - took over ops responsibilities
      - started using cassandra and kafka for scaling (asked permission, was
        refused, and did it anyway because they needed it)
      - in following two years, enabled fifty-three new business capabilities
      - eighty deployments per week
      - digital sales increased 42% during 2014
      - 280k in-store pickup orders in 2015 black friday
* How to get great outcomes by integrating operations into the daily work of development
** Create shared services to increase developer productivity
   - most of these services should be self service, without requiring submitting
     a ticket, so that operations doesn't become a bottleneck
   - "without these self-service operations platforms, the cloud is just
     Expensive Hosting 2.0"
   - examples
     - shared version control repository with pre-blessed security libraries
     - deployment pipeline that automatically runs code quality and security
       scanning tools
     - deploys into known, good environments that already have production
       monitoring tools installed
   - platform team can educate development teams and spread best practices and
     tools across teams so that it's easier for developers to move between teams
     without always having to relearn a new set of tools
** Embed ops engineers into our service teams
   - product teams often have the budget to fund hiring these ops engineers
   - these ops engineers are driven by team priorities, not overall priorities
     and internal ops concerns
   - efficient way to cross-train operations knowledge and expertise into
     service team
** Assign an ops liason to each service team
   - liason responsible for understanding:
     - what is the new product functionality and why we're building it
     - how it works as it pertains to operability, scalability, and observability
     - how to monitor and collect metrics to ensure the progress, success, or
       failure of the functionality
     - any departures from previous architectures and patterns, and the
       justifications for them
     - any extra needs for infrastructure and how usage will affect
       infrastructure capacity
     - feature launch plans
** Integrate ops into dev rituals
** Invite ops to our dev standups
** Invite ops to our dev retrospectives
** Make relevant ops work visible on shared kanban boards
   - put operations work relevant to product delivery on kanban board
* Part III: The First Way - The technical practices of flow
** Create the foundations of our deployment pipeline
   - production like environments at every stage of value stream
   - created in automated, on demand way
*** Enable on demand creation of dev, test, and production environments
    - use automation for any or all of the following
      - copying a virtualized environment (eg vmware image, running a vagrant
        script, booting an amazon machine image file in ec2)
      - building an automated environment creation process that starts from "bare
        metal" (eg PXE install from a baseline image)
      - using "infrastructure as code" configuration management tools (eg puppet,
        chef, ansible salt, cfeengine, etc)
      - using automated operating system configuration tools (eg solaris
        jumpstart, red hat kickstart, debian preseed)
      - assembling an environment from a set of virtual images or containers (eg
        vagrant, docker)
      - spinning up a new environment in a public cloud (eg amazon web services,
        google app engine, microsoft azure), private cloud, or other paas (platform
        as a service, such as openstack or cloud foundry, etc)
    - providing developers environment they fully control allows them to test
      changes to environment, get feedback quickly, etc
*** Create our single repository of truth for the entire system
    - put everything, including QA, operations, infosec, etc into revision control
    - check in the following assets to our shared version control repository
      - all application code and dependencies (eg libraries, static content, etc)
      - any script used to create database schemas, application reference data, etc
      - all environment creation tools and artifacts (eg vmware or ami images,
        puppet or chef recipes, etc)
      - any file used to create containers (eg docker or rocket definition or
        composition files)
      - all supporting automated tests and any manual test scripts
      - any script that supports code packaging, deployment, database migration,
        and environment provisioning
      - all project artifacts (eg requirements docs, deployment procedures,
        release notes, etc)
      - all cloud configuration files (eg aws cloudformation templates, microsoft
        azure stack dsc files, openstack heat)
      - any other script or configuration information required to create
        infrastructure that supports multiple services (eg enterprise service
        buses, database management systems, dns zone files, configuration rules
        for firewalls, and other networking devices)
    - must be able to recreate pre-production and build processes as well as
      production environments (ie tools, compilers, testing)
    - whether ops used version control higher predictor for it performance and
      organizational performance than whether dev used version control
      - why? orders of magnitude more configurable settings in environment than code
*** Make infrastructure easier to rebuild than to repair
    - immutable infrastructure
    - prevent uncontrolled configuration variances
      - disable remote logins to production servers
      - routinely kill or replace production instances
*** Modify our definition of development "done" to include running in production-like environments
** Enable Fast and Reliable Automated Testing
*** Continuously build, test, and integrate our code and environments
    - our build and test processes can run all the time, independent of the work
      habits of individual engineers
    - a segregated build and test process ensures that we understand all the
      dependencies required to build, package, run, and test our code (ie
      removing the "it worked on the developer's laptop, but it broke in
      production" problem)
    - we can package our application to enable the repeatable installation of
      code and configurations into our environment (eg on linux rpm, yum, npm;
      on windows, oneget; alternatively framework-specific packaging systems can
      be used, such as ear and war files for java, gems for ruby, etc)
    - instead of putting our code in packages, we may choose to package our
      applications into deployable containers (eg docker, rkt, lxd, amis)
    - environments can be made more production-like in a way that is consistent
      and repeatable (eg compilers are removed from the environment, debugging
      flags are turned off, etc)
    - pipeline stores history, including information about which tests were
      performed on which build, which builds have been deployed to which
      environment, and what the test results were
    - continuous integration practices
      - comprehensive and reliable set of automated tests that validate we are
        in a deployable state
      - a culture that "stops the entire production line" when our validation
        tests fail
      - developers working in small batches on trunk rather than long-lived
        feature branches
*** Build a fast and reliable automated validation test suite
    - nightly build not fast enough feedback
      - takes longer to find out what change broke the build
      - issue might have been caused by a test environment issue, which may only
        reappear the next night
      - people are checking in more changes with more potential breakers while
        we're fixing the current stuff
      - automated tests
        - unit tests: does what developer expects
        - acceptance tests: does what customer expects
        - integration tests: our application correctly interacts with other
          production applications and services. These tests are brittle and long
          running.
*** Catch errors as early in our automated testing as possible
    - find errors as early as possible
    - errors should be found with the fastest category of testing possible
    - finding/fixing integration test failures is painful, especially since the
      feedback loop can be hours
    - whenever we find an error with an acceptance/integration test, we should
      create a unit test that could find error faster, earlier, and cheaper
    - ideal testing automation pyramid
      - starting at bottom
      - automated unit tests
      - automated component tests
      - automated integration tests
      - automated api tests
      - automated gui tests
      - manual session based testing
    - non-ideal testing automation inverted pyramid
      - starting at bottom
      - manual test
      - automated gui test
      - integration test
      - unit test
    - if unit or acceptance tests are too difficult and expensive to write and
      maintain, likely we have architecture too tightly-coupled, where strong
      separation between module boundaries no longer exist. Need to create more
      loosely-coupled system so modules can be independently tested without
      integration environments
    - acceptance test suites for even most complex applications that run in
      minutes are possible
*** Ensure tests run quickly (in parallel, if necessary)
    - design tests to run in parallel, potentially across many different servers
    - run different categories of tests in parallel
    - for example, when build passes acceptance tests, may run our performance
      testing in parallel with security testing
    - may or may not allow manual exploratory testing until build has passed all
      automated tests - which enables faster feedback, but may also allow manual
      testing on builds that will eventually fail
*** Write our automated tests before we write code
    - ensure tests fail. Check in
    - ensure tests pass. Check in
    - refactor both new and old code to make it well structured. Ensure tests
      pass. Check in again
*** Automate as many of our manual tests as possible
    - by automating what tests we can, we enable all testers to work on
      high-value activities that cannot be automated, such as exploratory
      testing or improving the test process itself
    - we do not want automated tests that are unreliable or generate false positives
    - unreliable tests create significant problems
      - waste valuable time, increase overall effort of running and interpreting
        test results, and often lead to stressed developers ignoring test
        results entirely or turning off automated tests
      - to mitigate, a small number of reliable, automated tests almost always
        preferable over a large number of manual or unreliable automated tests
      - focus on automating only tests that genuinely validate business goals we
        are trying to achieve
*** Integrate performance testing into our test suite
    - detect conditions like
      - database query times grow non-linearly
      - code change causes number of database calls, storage use, or network
        traffic to increase ten-fold
      - code change causes number of database calls, storage use, network
        traffic to increase ten-fold
    - creating performance testing environment can easily be more complex than
      creating the production environment for application itself
    - may build performance testing environment at start of project and ensure
      that we dedicate whatever resources are required to build it early and correctly
    - should log performance results and evaluate each performance run against
      previous results
*** Integrate non-functional requirements testing into our test suite
    - availability, scalability, capacity, security, so forth
    - want to enforce consistency and correctness of following
      - supporting application, databases, libraries, etc
      - language interpreters, compilers, etc
      - operating systems (eg audit logging enabled, etc)
      - all dependencies
    - infrastructure as code configuration, can use same testing frameworks we
      use to test our code to also test our environments are configured and
      operating correctly (eg encoding environment tests into cucumber or
      gherkin tests)
    - should run tools that analyze code that constructs environments
      (foodcritic for chef, puppet-lint for puppet)
    - run any security hardening checks as part of automated tests
    - must create andon cord so that when someone breaks deployment pipeline, we
      take all necessary steps to get back into green build state
*** Pull our andon cord when deployment pipeline breaks
    - when someone introduces change that causes our build to fail, no new work
      is allowed to enter system until problem is fixed
    - if someone needs help to resolve problem, they can bring in whatever help
      they need
    - when pipeline broken, notify entire team, so anyone can fix or roll-back commit
    - may even configure version control system to prevent further code commits
      until first stage (builds and unit tests) of the pipeline is green
    - to increase visibility of automated test failures, we should create highly
      visible indicators so that entire team can see when build or tests are failing
    - this step more challenging than creating builds and test servers, since
      this requires changing human behavior and incentives
*** Why we need to pull the andon cord
    - if we don't pull the cord
      - someone checks in breaking code but no one fixes it
      - someone checks in another change onto broken build, but no one sees the
        failing test results which are hidden by first failures
      - our existing tests don't run reliably, so we are very unlikely to build
        new tests
    - when these things happen, deployments to any environment become as
      unreliable as when we had no automated tests or using waterfall method,
      where majority of our problems are discovered in production
*** Conclusion
    - set the stage for implementing continuous integration, which allows many
      small teams to independently and safely develop, test, and deploy code
      into production
** Enable and practice continuous integration
   - long running branches makes integration much harder
   - automated tests allow continuous integration without constantly breaking things
*** Small batch development and what happens when we commit code to trunk infrequently
    - people more reluctant to refactor because creates even more difficult merges
*** Adopt trunk-based development practices
    - gated commits - deployment pipeline confirms that submitted change will
      successfully merge, build as expected, and pass all automated tests before
      actually being merged into trunk. If not, developer will be notified
** Automate and enable low-risk releases
*** Automate our deployment process
    - document existing deployment to production process, such as:
      - packaging code in ways suitable for deployment
      - creating pre-configured virtual machine images or containers
      - automating the deployment and configuration of middleware
      - copying packages or files onto production servers
      - restarting servers, applications, or services
      - generating configuration files from templates
      - running automated smoke tests to make sure the system is working and
        correctly configured
      - running testing procedures
      - scripting and automating database migrations
    - reduce lead times and handoffs to reduce errors and loss of knowledge
    - development must work closely with operations to ensure all tools and
      processes we co-create can be used downstream
    - requirements for deployment pipeline include:
      - deploying the same way to every environment
      - smoke testing our deployments
      - ensure we maintain consistent environments
*** Enable automated self-service deployments
    - "as a developer, there has never been a more satisfying point in my career
      than when I wrote the code, when I pushed the button to deploy it, when I
      could see the production metrics confirm that it actually worked in
      production, and when I could fix it myself if it didn't"
    - not significant difference in reliability if dev or operations actually do deploys
    - to enable fast flow, want code promotion process that can be performed by
      either dev or operations, ideally without manual steps or handoffs. This
      affects following steps:
      - build: must create packages from version control that can be deployed to
        any environment, including production
      - test: anyone should be able to run any or all of our automated test
        suite on their workstation or on test systems
      - deploy: anybody should be able to deploy packages to any environment
        where they have access, executed by running scripts that are checked
        into version control
*** Integrate code deployment into deployment pipeline
    - our deployment automation must provide following capabilities:
      - ensure packages created during continuous integration are suitable for
        deployment into production
      - show readiness of production environments at a glance
      - provide push-button, self-service method for any suitable version of
        packaged code to be deployed into production
      - record automatically, for auditing and compliance purposes, which
        commands were run on which machines when, who authorized it, and what
        output was
      - run smoke test to ensure system is operating correctly and configuratin
        settings, including items such as database connection strings, are correct
      - provide fast feedback for deployer so they can quickly determine whether
        it was successful
    - case study: etsy - self-service developer deployment
      - run unit tests in parallel on multiple jenkins instances
      - run smoke tests and gui driven tests
*** Decouple deployments from releases
    - deployment is installation of specified version of software into given environment
    - release is when we make a feature or set of features available to all our
      customers or segment of customers. Our code and environments should be
      architected in such a way that release of functionality does not require
      changing our application code
    - as we deploy faster, how quickly we expose new functionality becomes
      business and marketing decision, not technical decision
    - two broad categories of release patterns
      - *environment based release patterns*: we have two or more environments
        we deploy into, but only one receiving live traffic (via load
        balancers). New code deployed into non-live and release is performed
        moving traffic to environment.
        - blue-green deployments
        - canary releases
        - cluster immune systems
      - *application-based release patterns*: modify application so we
        selectively release and expose specific application functionality by
        small configuration changes. Feature flags that progressively expose new
        functionality in production to devs, all internal employees, 1% of
        customers, or everyone
        - enables dark launching, where we stage all functionality to be
          launched in production and test it with production traffic
**** environment based release patterns
***** blue green
      - two production systems, blue and green
      - at any time, only one serving customer traffic
      - deploy to inactive system, test it out, then redirect traffic
      - rollback is performed by redirecting to the old system
****** Dealing with database changes
       - two approaches
         - create two databases
           - blue (old) and green (new)
           - during release, put blue database into read-only mode, perform
             backup, restore onto green, and finally switch traffic
           - problem is if we need to roll back to blue version, we can lose
             transactions if we don't manually migrate from green
         - decouple database changes from application changes
           - release of database changes separate from release of application changes
           - make only additive changes to database, never mutating existing
             database objects
           - no assumptions in application about which database version will be
             in production
****** Case study: Dixons retail - blue-green deployment for point-of-sale system
       - typically upgrading POS systems are big bang waterfall
       - weeks before planned POS upgrade, they started sending out new versions
         of client POS software installers to retail stores over slow network
         links, deploying new software in inactive state
       - when all clients staged (upgraded client and server had tested together
         succesfully, and new client software had been deployed to all clients),
         store managers were empowered to decide when to release new version
       - depending on business needs, some managers wanted to use new features
         immediately while others wanted to wait
***** Canary and Cluster IMmune System release patterns
      - canary release pattern automates release process of promoting to
        successively larger and more critical environments
        - miners brought in canaries to see if too much gas min mine
        - when we release, monitor how software in environment is performing
        - when something wrong, roll back. Otherwise go to next environment
      - cluster immune system
        - expands upon canary pattern by linking production monitoring system
          with release process and automating roll back of code when user-facing
          performance of production system deviates outside predefined expected
          range, such as conversion rates for new users drops below historical
          norms of 15-20%
        - protects against defects that are hard to find through automated tests
          and reduces time required to detect and respond to degraded performance
**** Application-based patterns to enable safer releases
***** Implement feature toggles
      - selectively enable and disable features without requiring production
        code deployment
      - usually implemented by wrapping application logic or UI elements with
        conditional statement, where feature is enabled or disabled based on configuration
      - allows us to
        - roll back easily
        - gracefully degrade performance: when service experiences extremely
          high loads that would normally require increased capacity or cause
          failures, we can use feature toggles to reduce quality of service
          (reduce number of customers who can access a certain feature, disable
          CPU-intensive features, etc)
        - increase our resilience through service-oriented architecture: if we
          have feature that relies on another service that isn't complete yet,
          we can still deploy feature into production but hide it behind feature toggle
      - to ensure we find errors in features, automated acceptance tests should
        run with all features on (also test that feature toggling functionality
        works)
***** Perform dark launches
      - deploy all functionality into production then perform testing of
        functionality while still invisible to customers
      - for large/risky changes, do this for weeks before launch, enabling us to
        safely test with anticipated production loads
      - dark launch with feature disabled, modify user session code to make
        calls to new functions, but instead of displaying results, simply log or
        discard results
      - may have 1% or users make invisible calls to new feature scheduled to be
        launched to see how new feature behaves under load. Progressively
        increase simulated load by increasing frequency and number of users
        exercising new functionality
      - when launch feature, progressively roll out feature to small segments of
        customers, halting release if any problems found
      - when we have adequate production telemetry, can enable faster feedback
        cycles to validate business assumptions and outcomes immediately
      - no longer big bang - by time announce and release feature, have already
        tested business hypotheses and run countless experiments with real customers
****** Case study: dark launch of facebook chat (2008)
       - deployed every day into production for a year, but dark launch, to
         find/fix performance issues
** Survey of continuous delivery and continuous deployment in practice
   - deployments should be low-risk, push-button events we can perform on demand
   - when trunk is always in releasable state and we can release on demand at
     push of a button, doing continuous delivery
   - when deploying good builds into production on regular basis through
     self-service, engaging in continuous deployment
* 13: Architect for low-risk releases 
  - strangler application pattern - instead of ripping out and replacing old
    services with architectures that no longer support organizational goals, put
    existing functionality behind API and avoid making further changes. All new
    functionality is implemented in new services that use desired architecture,
    making calls to old system when necessary
    - especially useful migrating portions of monolithic application or tightly
      coupled services to one that is more loosely coupled
    - consequences of overly tight architectures
      - every time commit code to trunk, risk creating global failures (break
        everyone else's tests or entire site goes down)
  - "IT Project owners are not held accountable for their contributions to
    overall system entropy". Reducing overall complexity and increasing
    productivity is rarely goal of individual project
** An architecture that enables productivity, testability, and safety
   - loosely coupled architecture with well-defined interfaces that enforce how
     modules connect with each other promotes productivity and safety
   - enables small, productive, two-pizza teams able to make small changes that
     can be safely and independently deployed
   - each service has well-defined API, enabling easier testing of services and
     creation of contracts and SLAs between teams
** Architectural archetypes: monoliths vs microservices
   - monoliths fast to build but hard to operate at scale
   - no perfect architecture. Any architecture meets particular set of goals or
     range of requirements and constraints, such as time to market, ease of
     developing functionality, scaling, etc. Architecture will almost certainly
     evolve over time as needs change
   - monolithic v1 (all functionality in one application)
     - pros
       - simple at first
       - low inter-process latency
       - single codebase, one deployment unit
       - resource efficient at small scale
     - cons
       - coordination overhead increases as team grows
       - poor enforcement of modularity
       - poor scaling
       - all-or-nothing deploy (downtime, failures)
       - long build times
   - monolithic v2 (sets of monolithic tiers: "front end presentation",
     "application server", "database layer")
     - pros
       - simple at first
       - join queries are easy
       - single schema, deployment
       - resource-efficient at small scale
     - cons
       - tendency for increased coupling over time
       - poor scaling and redundancy (all or nothing, vertical only)
       - difficult to tune properly
       - all-or-nothing schema management
   - microservice (modular, independent, graph relationship vs tiers, isolated persistence)
     - pros
       - each unit simple
       - independent scaling and performance
       - independent testing and deployment
       - can optimally tune performance (caching, replication, etc)
     - cons
       - many cooperating units
       - many small repos
       - requires more sophisticated tooling and dependency management
       - network latencies
*** Case study: evolutionary architecture at amazon (2002)
    - amazon started in 1996 as monolithic application, running on web server,
      talking to database
    - time went on, application grew too tangled, with complex sharing
      relationships meaning individual pieces could not be scaled
    - bit architectural change in past five years (2001-2005) was to move from
      two-tier monolith to fully distributed, decentralized, services platform
      serving many different applications
    - lessons
      - lesson 1: when applied rigorously, strict service orientation is
        excellent technique to achieve isolation; level of ownership and control
        not seen before
      - lesson 2: prohibiting direct database access by clients makes
        performing, scaling, and reliability improvements to service state
        possible without involving clients
      - lesson 3: development and operational process greatly benefits from
        switching to service-orientation. Services model key enabler for teams
        innovating quickly with strong customer focus. Each service has team and
        team is completely responsible for service, from scoping out
        functionality to architecting, building, and operating
    - in 2011, amazon performing 15000 deployments per day
    - 2015, 136,000 deployments per day
** Use the strangler application pattern to safely evolve our enterprise architecture
   - place existing functionality behind API, where it's unchanged, implementing
     new functionality using desired architecture, making calls to old system
     when necessary
   - all services accessed through versioned APIs called versioned services or
     immutable services
   - versioned APIs enable modification of service without impacting callers,
     which allows system to be more loosely coupled. If we need modify
     arguments, create new API version and migrate teams who depend on service
     to new version
   - if services do not have cleanly defined APIs, we should build them or at
     least hide complexity of communicating with such systems within a client
     library that has cleanly defined API
   - by repeatedly decoupling functionality from tightly-coupled system, move
     our work into safe and vibrant ecosystem where developers can be far more
     productive resultin in legacy application shrinking. Might even disappear
     as all needed functionality migrates to new architecture
   - with strangler applications, we avoid reproducing existing functionality in
     some new architecture - often business processes far more complex than
     necessary due to idiosyncrasies of existing systems, which we end up replicating
   - by researching user, can often re-engineer process so that we can design
     far simpler and more streamlined means to achieving business goal
   - martin fowler: "much of my career has involved rewrites of critical
     systems. You'd think it's easy, just make new one do what old one did. Yet
     always much more complex than they seem, and overflowing with risk. Big
     cut-over date looms, and pressure is on. While new features (always new
     features) are liked, old stuff has to remain. Evven old bugs often need to
     be added to rewritten system"
   - seek to create quick wins and deliver early incremental value before
     continuing to iterate
*** Case study: strangler pattern at blackboard learn (2011)
    - dealing with consequences of legacy j2ee codebase that went back to 1997
    - in 2010, build, integration, and testing processes getting more and more
      complex and error prone
    - larger product got, longer lead times and worse outcomes for customers
    - to even get feedback from integration process required twenty four to
      thirty six hours
    - saw that amount of code kept growing, but commits were dropping off due to
      difficulty of introducing change
    - in 2010, used strangler pattern
    - created building blocks, which allowed developers to work in separate
      modules that were decoupled from monolithic codebase and accessed through
      fixed APIs
    - when building blocks made available to developers, size of monolithc
      source code began to decrease as developers moved code into building block
      modules source code
    - allowed deveopers to be more productive because work safer with mistakes
      resulting in small, local failures instead of major catastrophes
** Conclusion
   - architecture one of top predictors of productivity of engineers that work
     within it
* Part IV: The Second Way, The Technical Practices of Feedback
** Create telemetry to enable seeing and solving problems
   - often can't tell root cause when outage in production
   - standard response is start rebooting things
   - best organizations diagnose and fix issues - "culture of causality"
   - need telemetry to accomplish this - "an automated communications process by
     which measurements and other data are collected at remote points and are
     subsequently transmitted to receiving equipment for monitoring"
   - need telemetry in applications, environments, and deployment pipeline
   - etsy used graphite to track metrics, overlaid over every metric graph when
     deployments happened to see how they affected things
   - high performers resolve production incidents 168 times faster than peers
   - top two technical practices enabling fast mean time to repair (MTTR)
     - use of version control by operations
     - telemetry and proactive monitoring in production
*** Create our centralized telemetry infrastructure
    - must have holistic metrics that show how whole system is behaving,
      applications and environment
    - need graphing, visualizing, anomaly detection, proactive alerting and
      escalation, etc
    - The Art of Monitoring, by James Turnbull, describes monitoring architecture
      using open source tools that could be customized and deployed at scale
      - data collection at the business logic, application, and environment layer
        - creating telemetry with events, logs, and metrics
        - logs may be stored on each server but preferably want all logs sent to
          common service that enables easy centralization, rotation, deletion (eg syslog)
        - gather metrics at all levels
          - operating system
            - cpu
            - memory
            - disk
            - network usage
      - event router responsible for storing events and metrics
        - enables visualization, trending, alerting, anomaly detection
        - where configuration related to services stored and where
          threshold-based alerting and health checks happen
    - with centralized logs, can turn metrics into counting/summarization
    - log event such as "child pid 14024 exit signal segmentation fault" can be
      counted and summarized as single segfault metric
    - could configure alert if we go from "ten segfaults last week" to "thousands
      of segfaults in last hour"
    - also need telemetry from deployment pipeline, such as when tests pass/fail
      or how long performance tests take to run
    - should be self service to enter and retrieve information from telemetry
      infrastructure, as opposed to tickets and waiting for reports
    - Adrian Cockcroft "Monitoring is so important that our monitoring systems
      need to be more available and scalable than the systems being monitored"
*** Create application logging telemetry that helps production
    - dev and ops engineers must create production telemetry as part of daily
      work, both for new and existing services
    - in applications, every feature should be instrumented
    - devs may temporarily add more telemetry to application to diagnose problems
      on their workstation
    - ops may use telemetry to diagnose production problem
    - infosec and auditors may use telemetry to confirm required control
    - production manager may use to track business outcomes, feature usage, or
      conversion rates
    - different logging levels for different usage
      - debug: anything that happens in program, can be used in production
        temporarily for troubleshooting
      - info: actions that are user-driven or system specific
      - warn: conditions that could be potentially become error (eg database call
        taking longer than some predefined time)
      - error
      - fatal: tells us when we must terminate (eg network daemon can't bind a
        network socket)
    - "when deciding whether a message shoud be error or warn, imagine being
      woken up at 4 am. Low printer toner is not an error"
    - list of events that should generate logging entries
      - authentication/authorization decisions (including logoff)
      - system and data access
      - system and application changes (especially privileged changes)
      - data changes, such as adding, editing, deleting data
      - invalid input (possible malicious injection, threats, etc)
      - resources (ram, disk, cpu, bandwidth, or other resource with hard or soft limits)
      - health and availibility
      - startups and shutdowns
      - faults and errors
      - circuit breaker trips
      - delays
      - backup success/failure
    - to make easier to interpret logs, should create logging hierarchical
      categories, such as non-functional (eg performance, security) and
      attributes related to features (eg search, ranking)
*** Use telemetry to guide problem solving
    - without telemetry must rely on rumor and hearsay, which leads to
      unfortunate metric of mean time until declared innocent - how quickly can
      we convince everyone that we didn't cause outage
    - where culture of blame around outages, groups may avoid documenting changes
      and displaying telemetry to avoid blame
    - other negative outcomes from no public telemtry include highly charged
      political atmosphere, need to deflect accusations, and inability to create
      institutional knowledge around how incidents occurred and learnings needed
      to prevent in future
    - questions we can answer with telemetry during troubleshooting
      - what evidence does monitoring have that problem is occurring?
      - what are relevant events and changes in applications/environments that
        could have contributed to problem?
      - what hypotheses can we formulate to confirm link between proposed causes
        and effects?
      - how can we prove hypotheses and successfully fix?
*** Enable creation of production metrics as part of daily work
    - need to make it easy for everyone to create metrics that can be created,
      displayed, and analyzed
    - must create infrastructure and libraries
    - ideally, one line of code should create new metric that shows up in common
      dashboard where everyone can see it
    - this philosophy guided stasd, created and open sourced at etsy
    - when generate telemetry graphs, also overlay when production changes occur,
      such as code deployments
    - other libraries include JMX and codehale metrics. Other tools New Relic,
      AppDynamics, Dynatrace, munin, and collectd
*** Create self-service access to telemetry and information radiators
    - after creating infrastructure, must tell everyone in organization about how
      to use it
    - put information in central areas where dev and ops work, allowing everyone
      to see how services are performing
    - information radiator "generic term for any number of handwritten, drawn,
      printed, or electronic displays which a team places in highly visible
      location, so that all team members as well as passers-by can see latest
      information at a glance: count of automated tests, velocity, incident
      reports, continuous integration status, and so on"
    - information radiators promote
      - team has nothing to hide from visitors
      - team has nothing to hide from itself: it acknowledges and confronts problems
    - might even broadcast telemetry to internal/external customers to build trust
**** Case study: creating self-service metrics at linkedin (2011)
     - in 2010, even though lots of telemetry, very difficult for engineers to
       get access and analyze
     - made changes to make it more accessible and usefulness increased significantly
*** Find and fill any telemetry gaps
    - need metrics for
      - business level: sales transactions, revenue, user signups, churn rate,
        A/B testing results
      - application level: transaction times, user response times, application faults
      - infrastructure level: web server traffic, cpu load, disk usage
      - client software level: application errors and crashes, user measured
        transaction times
      - deployment pipeline level: build pipeline status (red/green), change
        deployment lead times, deployment frequencies, test environment
        promotions, environment status
    - telemetry better informs security as well
    - by detecting and correcting problems earlier, can fix them while they are
      small and easy to fix, with fewer customers impacted
*** Application and business metrics
    - instrument all user actions required for desired customer outcomes
    - for e-commerce sites, how much time users spent on site
    - for search engines, reduce time spent on site
    - business metrics part of customer acquisition funnel, which is theoretical
      steps potential customer will take to make purchase
    - in e-commerce, measurable journey events include total time on site,
      product link clicks, shopping cart adds, and completed orders
    - goal is every business metric be actionable - top metrics should inform how
      product changes and be amenable to experimentation and A/B testing
    - anyone viewing information radiators should be able to make sense of
      information in context of desired organizational outcomes, such as goals
      around revenue, user attainment, conversion rates, etc
    - should define and link each metric to business outcome metric at earliest
      stages of feature definition and development, and measure outcomes after in production
    - business context can be created by being aware of/visually displaying time
      periods relevant to high-level business planning/operations, such as high
      transaction periods during holidays, end of quarter financial close
      periods, or scheduled compliance audits. Information may be used as
      reminder to avoid scheduling risky changes when availability is critical or
      avoid certain activities when audits in progress
*** Infastructure metrics
    - when something goes wrong with environment, need to know what
      applications/services affected
    - use tools like Zookeeper, Etcd, Consul, etc which let services register
      themselves and store information about what other services they depend on
    - business metrics create context for infrastructure metrics - "instead of
      measuring operations against amount of downtime, better to measure against
      how much revenue we lost"
*** Overlaying other relevant information onto our metrics
    - even after pipeline and small frequent changes, still risky
    - for instance, service with large number of transactions, production changes
      can result in significant settling period, where performance degrades
      substantially as all cache lookups miss
    - overlay onto graphs things like deployment, when service under maintenance
      or being backed up
** Analyze telemetry to better anticipate problems and achieve goals
   - for example, netflix gathers information about all its servers and then
     looks to find outliers who don't fit the normal pattern and replaces those
     outliers with new servers
*** Use means and standard deviations to detect potential problems
    - "alert fatigue is single biggest problem right now...Need to be more
      intelligent about alerts or we'll all go insane"
*** Instrument and alert on undesired outcomes
    - find alerts that could have enabled faster detection/diagnosis of actual
      incidents in the recent past
    - for example, if NGINX web server stopped responding to requests, what
      indicators could warn earlier
      - application level: increasing web page load times, etc
      - OS level: server free memory running low, disk space running low, etc
      - database level: database transaction times taking longer than normal, etc
      - network level: number of functioning servers behind load balancer
        dropping, etc
    - by repeating this process on ever-weaker failure signals, find problems
      earlier in life cycle
*** Problems that arise when our telemetry data has non-gaussian distribution
**** Case study: auto-scaling capacity at netflix
     - netflix developed tool scryer which predicts what customer load will be
       based on historical usage patterns and preemptively provisions necessary capacity
     - addresses three problems with AAS (amazon auto scaling)
       - rapid spikes in demand cannot have new servers added quickly enough
       - after outages, decrease in demand led to too many servers being removed
       - AAS didn't factor in known usage traffic patterns when scheduling
         compute capacity
     - scryer uses combination of outlier detections to discard spurious data
       points and then uses things like Fast Fourier Transform (FFT) and linear
       regression to smooth data while preserving legitimate traffic spikes
     - months after scryer in production, netflix greatly improved customer
       viewing experience, improved service availability, and reduced amazon EC2 costs
**** Using anomaly detection techniques
     - some companies have ops engineer trained in statistics (R code) to detect
       anomalies earlier and earlier
     - techniques
       - smoothing
         - suitable if data is time series, meaning each data point has time stamp
         - uses moving averages to transform data by averaging each point with
           all other data within sliding window
       - fast fourier transforms
         - image transforms
       - kolmogorov-smirnov
         - find similarities/differences in periodic/seasonal data
**** Case study: advanced anomaly detection
**** Tools
     - Oculus: finds graphs with similar shapes which may indicate correlation
     - Opsweekly: tracks alert volumes and frequencies
     - Skyline: identifies anomalous behavior
** Enable feedback so development and operations can safely deploy code
   - telemetry in production gives developers confidence to deploy
** Use telemetry to make deployments safer
   - monitor telemetry during deployments
   - disable bad code with feature toggles, fix forward, or roll back
   - deployment and change events overlaid onto metric graphs
** Dev shares pager rotation duties with ops
   - put developers, managers, architects on pager rotation to ensure everyone
     getting feedback from architectural/coding decision
   - side effect is that development management see that business goals are not
     achieved simply because feature marked as "done". Feature only done when
     performing as designed in production
** Have developers follow work downstream
   - contextual inquiry: product team watches customer use application in
     natural environment
   - feel customers pain in the UX
** Have developers initially self-manage their production service
   - development groups self-manage services in production before they become
     eligible for centralized ops group to manage
   - more likely to not dump bunch of operational issues onto operations group
   - define launch requirements that must be met in order for services to
     interact with real customers
   - ops engineers should act as consultants
   - launch guidance/requirements
     - defect counts and severity
     - type/frequency of pager alerts
     - monitoring coverage
     - system architecture
     - deployment process
     - production hygiene
   - if deficiencies found during review, assigned ops engineer should help resolve
   - learn about regulatory compliance objectives
     - does service generate significant amount of revenue?
     - does service have high user traffic?
     - does service store payment cardholder information? other security issues
       that could create regulatory, contractual obligation, privacy, or
       reputation risk?
     - does service have other regulatory or contractual compliance
       requirements, such as export regulations, PCI-DSS, HIPAA, etc?
   - if service already in production a while, may use service handback
     mechanism when service becomes sufficiently fragile. Operations can return
     production support responsibility to development
*** Case study: the launch and hand-off readiness review at google
    - Site Reliability Engineers - software engineer tasked with operations
    - Launch Readiness Review must be signed off on before new service made
      publicly available
    - Hand-Off Readiness Review performed when service transitioned to
      ops-managed state, months after LRR
    - SRE assigned to team to help with LRR and HRR
    - in best case, product teams use LRR checklist as guideline, working on
      fulfilling it while developing service and reaching out to SREs when need help
** Integrate hypothesis-driven development and A/B testing into our daily work
   - many times, developers work on features months/years over multiple releases
     without confirming whether desired business outcomes being met
   - when we discover feature isn't achieving desired results, making correction
     may be out-prioritized by new features
   - "most inefficient way to test a business model or product idea is to build
     the complete product to see whether the predicted demand actually exists"
   - before build feature, "should we build it, and why?"
   - then do fastest, cheapest experiments to validate through user research
     whether intended feature will achieve desired outcome
** A brief history of A/B testing
   - developed during direct response marketing, which sends some sort of
     message directly to people asking for response
   - tested many different styles of message to see which more effective
** Integrating A/B testing into our feature testing
   - example: website where visitors randomly shown one of two versions of page,
     either control (A) or treatment (B). Based on statistical analaysis of
     subsequent behavior, demonstrate whether significant difference in outcomes
     of two, establishing causal link between treatment (change in feature,
     design element, background color) and outcome (conversion rate, average
     order size)
   - also can run experiments with more than one variable, known as multivariate testing
   - "taken to extreme, the organization and customers would have been better
     off giving entire team vacation, instead of building one of these
     non-value-adding features"
** Integrate A/B testing into our release
   - fast, iterative A/B testing made possible by being able to quickly and
     easily do production deployments on demands, using feature toggles
   - etsy open-sourced experimentation framework (Feature API)
   - other A/B tools are Optimizely, Google Analytics, etc
** Integrating A/B testing into feature planning
   - once infrastructure in place, product owners must think about each feature
     as hypothesis and use testing to prove/disprove
     - *we believe* increasing size of hotel images on booking page
     - *will result* in improved customer engagement and conversion
     - *we will have confidence to proceed when* we see a 5% increase in
       customers who review hotel images who then proceed to book in forty-eight hours
   - adopting experimental approach requires us to break down work into small
     units and validate whether each unit of work is delivering expected outcomes
   - if not, modify future plan
*** Case study: doubling revenue growth through fast release cycle experimentation at yahoo! answers
    - yahoo answers had approximately 140 million monthly visitors, with over
      twenty million active users answering questions in twenty languages
    - however, user growth and revenue had flattened
    - after increasing speed of deployment from 4 weeks to multiple times a
      week, able to rapidly experiment
    - resulted in increased monthly visits of 72%, increased user engagement of
      threefold, and team doubled revenue
    - to continue success, team focused on optimizing top metrics:
      - time to first answer: how quickly was answer posted to user question?
      - time to best answer: how quickly user community award best answer?
      - upvotes per answer: how many times answer upvoted?
      - answers/week/person: how many answers users creating?
      - second search rate: how often did visitors have to search again for answer?
** Create review and coordination processes to increase quality of our current work
   - peer review process at github increases quality, makes deployments safe,
     and integrates into flow of everyone's daily work. Uses pull requests
   - pull requests used to deploy code through collective set of practices
     called "github flow" - how engineers request code reviews, gather and
     integrate feedback, and and announce that code will be deployed to production
   - five steps
     - to work on something new, engineer creates descriptively named branch off master
     - engineer commits to branch locally, regularly pushing work to same branch
       on server
     - when need feedback/help, or when they ready to merge, open a pull request
     - when get desired reviews and approvals, engineer can merge into master
     - once code changes merged/pushed to master, engineer deploys into production
   - goal to ensure development, operations, and infosec continuously
     collaborating so changes to systems reliable, secure, safe, as designed
** The dangers of change approval processes
   - knight capital failure was when fifteen minute deployment error resulted in
     $440 million trading loss, during which engineering teams unable to disable
     production services
   - forced company to be sold over weekend
   - for high profile incidents, typically two counterfactual narratives for why
     accident occurred
   - first narrative that accident due to change control failure
   - second narrative accident due to testing failure
   - in environments with low trust, command and control cultures, outcomes of
     change control and testing countermeasures often result in more problems
   - building high trust cultures likely largest management challenge of decade
** Potential dangers of "overly controlling changes"
   - can contribute to long lead times, reducing strength and immediacy of
     feedback from deployment process
   - change control often put in place
     - adding more questions that need to be answered to change request form
     - requiring more authorizations from management or stakeholders
     - requiring more lead time for change approvals so change requests can be
       properly evaluated
   - core belief of toyota production system "people closest to problem
     typically know most about it", which more and more true the more
     complex/dynamic work is
   - further distance between person doing work and person deciding to do work,
     worse the outcome
   - high-performing organizations rely more on peer review and less on external
     approval of changes
** Enable coordination and scheduling of changes
   - service oriented, loosely coupled architectures enable less communication/coordination
   - however, still risk of interfering with each other (eg simultaneous A/B tests)
   - may use chat rooms to announce changes and find collisons
   - for complex organizations and more tightly coupled architectures, may need
     deliberately schedule changes, where team representatives schedule and
     sequence changes
   - certain areas, like global infrastructure, always have higher risk and
     require technical countermeasures like redundancy, failover, comprehensive
     testing, and ideally simulation
** Enable peer review of changes
   - instead of external review, may require peer reviews
   - usually prior to committing to trunk
   - for higher risk areas, like database changes or business critical
     components, may require review from subject matter expert
   - non-linear relationship between size of change and risk of integrating change
   - essential to work in small incremental steps
   - "ask a programmer to review ten lines of code, he'll find ten issues. Ask
     him to do five hundred lines, and he'll say it looks good"
   - guidelines for review
     - everyone must have someone review changes before committing to trunk
     - everyone should monitor commit stream of team members so potential
       conflicts can be identified
     - define which changes qualify as high risk and require review from subject
       matter expert
     - if someone submits change that is too large to reason about easily,
       should be split into multiple smaller changes that can be understood at glance
   - should inspect code review statistics to determine number of proposed
     changes approved versus not approved
   - various forms
     - pair programming
     - over the shoulder: one developer looks over author's shoulder as latter
       walks through code
     - email pass-around: revision control system emails code to reviewers
       automatically after code checked in
     - tool assisted code review: things like github pull requests, more
       formalized flow in tool
*** Case study: code reviews at google (2010)
    - trunk based development requires discipline and code reviews which cover:
      - code readability for languages (enforces style guide)
      - ownership assignments for code sub-trees to maintain consistency and correctness
      - code transparency and code contributions across teams
** Potential dangers of doing more manual testing and change freezes
   - slows down deployment
   - need to fully integrate testing into daily work as part of smooth and
     continual flow into production
** Enable pair programming to improve all our changes
   - increases communication and learning
   - better, simpler designs
   - reduced bugs
*** Case study: Pair Programming replacing broken code review processes at pivotal labs (2011)
    - had to wait often week for review
    - senior engineers always reviewing and didn't have enough time
    - started requiring paired programming to avoid time cost of peer review
    - code reviews can work but requires culture that values reviewing code as
      highly as it values writing code
** Evaluating effectiveness of pull request processes
   - look at production outages and examine peer review process for relevant changes
   - bad pull request doesn't have enough context for reader
   - good pull request must have sufficient detail on why change is being made,
     how change was made, as well as identified risks and resulting countermeasures
