* The Three Ways
** The First Way: Flow
** The Second Way: The Principles of Feedback
   - In complex systems, no way to achieve perfection
   - Next best thing is detecting something gone wrong early to avoid a catastrophe
   - Treat failure as learning opportunity, not chance to play blame game
*** Working Safely Within Complex Systems
    - Complex work is managed so that problems in design and operations are revealed
    - Problems are swarmed and solved, resulting in quick construction of new knowledge
    - new local knowledge is exploited globally throughout the organization
    - leaders create other leaders who continually grow these types of capabilities
*** See problems as they occur
    - feedback loops
    - automated build, integration, and test
    - production telemetry to determine operational health
*** Swarm and solve problems to build new knowledge
    - steps
      - if something goes wrong or work is taking too long, alert the team
        leader
      - if the problem isn't resolved within a specified time, have rest of team
        work until problem is resolved
    - necessary because
      - prevents the problem from progressing downstream, where the cost and
        effort to repair it increases exponentially and technical debt is
        allowed to accumulated
      - prevents work center from starting new work, which will likely introduce
        new errors into the system
      - if problem is not addressed, the work center could potentially have the
        same problem in the next operation (e.g., fifty-five seconds later),
        requiring more fixes and work
    - need equivalent to andon cord
      - production incident
      - breaking change in continuous build or test processes
*** keep pushing quality closer to the source
    - examples of ineffective quality controls include:
      - requiring another team to complete tedious, error-prone, and manual
        tasks that could be easily automated and run as needed by the team who
        needs the work performed
      - requiring approvals from busy people who are distant from the work,
        forcing them to make decisions without an adequate knowledge of the work
        or the potential implications, or to merely rubber stamp their approvals
      - creating large volumes of documentation of questionable detail which
        become obsolete shortly after they are written
      - pushing large batches of work to teams and special committees for
        approval and processing and then waiting for responses
    - instead, everyone in value stream should find and fix problems in their
      area of control as part of daily work
    - use peer reviews
    - automate QA and IA as much as possible
    - make quality everyone's responsibility
*** enable optimizing for downstream work centers
    - *according to Lean, our most important customer is our next step downstream*
      - for example, the lab SAs and the CIT/SAT testers
    - operational non-functional requirements (e.g. architecture, performance,
      stability, testability, configurability, and security) are prioritized as
      highly as user features
** The Third Way: the principles of continual learning and experimentation
   - low performing environments discourage making mistakes and learning
   - high performing environments allow workers in the value stream to
     experiment with new improvements, enabled by rigorous standardization of
     work procedures and documentation of the results
   - reserve time for improvement of daily work and learning
   - introduce stress into systems to force continual improvement
   - simulate and inject failures in production services under controlled
     conditions to increase resilience
*** enabling organizational learning and a safety culture
    - when bad things happen, root cause analysis often leads to blame game and
      more top down processes and approvals
    - "Responses to incidents and accidents that are seen as unjust can impede
      safety investigations, promote fear rather than mindfulness in people who
      do safety-critical work, make organizations more bureaucratic rather than
      more careful, and cultivate professional secrecy, evasion, and self-protection"
    - three types of culture:
      - Pathological organizations are characterized by large amounts of fear
        and threat. People often hoard information, withhold it for political
        reasons, or distort it to make themselves look better. Failure is often hidden.
      - Bureaucratic organizations are characterized by rules and processes,
        often to help individual departments maintain their "turf". Failure is
        processed through a system of judgment, resulting in either punishment
        or justice and mercy.
      - Generative organizations are characterized by actively seeking and
        sharing information to better enable the organization to achieve its
        mission. Resposibilities are shared throughout the value stream, and
        failure results in reflection and genuine inquiry.
*** institutionalize the improvement of daily work
    - "even more important than daily work is the improvement of daily work"
*** transform local discoveries into global improvements
    - make blameless post-morten reports searchable by teams trying to solve
      similar problems
    - create shared source code repositories that span the entire organization
*** inject resilience patterns into our daily work
    - antifragility (Nassim Nicholas Taleb)
    - reduce deployment lead times
    - increase test coverage
    - decrease test execution times
    - re-architect if necessary to increase developer productivity or increase reliability
    - game day exercises, where we rehearse large scale failures, such as
      turning off entire data centers
    - inject ever larger scale faults into production environment
*** leaders reinforce a learning culture
    - explicitly state True North goals, such as "sustain zero accidents"
    - strategic goals inform iterative, shorter term goals
    - coaching questions for people conducting experiment
      - what was your last step and what happened?
      - what did you learn?
      - what is your condition now?
      - what is your next target condition?
      - what obstacle are you working on now?
      - what is your next step?
      - what is your expected outcome?
      - when can we check?
* Initiating a DevOps transformation
** Selecting which value streams to start with
   - example: Nordstrom
     - chose projects to improve who were already struggling to meet business goals
     - mobile app
       - initial state
         - could only release updates twice a year
         - customers were frustrated with product
       - improvement
         - first, enable faster on-demand releases
         - dedicated product team able to independently implement, test, and deliver
         - continuous planning -> single prioritized backlog of work
         - interated testing as daily activity
       - results
         - doubled features delivered per month
         - halved number of defects
     - systems supporting in store restaurants
       - business need
         - decrease cost
         - improve quality
       - initial state
         - in 2013, eleven "restaurant re-concepts" and forty-four more in 2014
         - business leaders suggested tripling team but they decided to improve process
       - improvement
         - work intake and deployment process
         - reduce code deployment times by 60%
         - reduced number of production incidents 60% to 90%
** Greenfield vs Brownfield Services
   - brownfield is well suited for devops
   - example: etsy
     - 35 employees and $87 million in revenue
     - barely survived holiday season and transformed all aspects of the organization
     - became highly admired devops organization and set the stage for a
       successful 2015 IPO
** Consider both systems of record and systems of engagement
   - systems of record
     - systems that run our business
     - MRP, HR, financial reporting
     - correctness of transactions/data are paramount
     - slower pace of change
   - systems of engagement
     - customer or employee facing
     - e-commerce, productivity, etc
     - higher pace of change
   - traditional wisdom says to use these categories to have a "fast
     experimental" and "slow safe" lane
   - however, devops has found that faster also can mean safer, because we're
     getting feedback and fixing problems faster
   - also, because a lot of systems are very interdependent, the ability to make
     changes to any system is often limited by the system that is the most
     difficult to safely change (usually a system of record)
   - scott prugh, VP of product development at CSG, says "We've adopted a
     philosophy that rejects bi-modal IT, because every one of our customers
     deserve speed and quality. This means that we need technical excellence,
     whether the team is supporting a 30 year old mainframe application, a Java
     application, or a mobile application"
** Start with the most sympathetic and innovative groups
** Expanding devops across our organization
   - demonstrate early wins and broadcast success by breaking  up large
     improvement goals into small, incremental steps
   - ideal  phases for change agents to build and expand coalition and support:
     - find innovators and early adopters
       - ideally,  respected  with high degree  of influence
     - build  critical mass and silent majority
     - identify the holdouts
       - high profile, influential detractors
       - tackle only after achieving silent majority
* Understanding the work in our value stream, making it visible, and expanding it across the organization
** Identifying the teams supporting our value stream
   - product owner
   - development
   - qa
   - operations
   - infosec
   - release managers
   - technology  executives or value stream manager
** Create a value stream map to see the work
   - document that answers question "how is work performed?"
   - example
     - customer request or business hypothesis coming through product owner
     - development  implements  at some point
     - build integrated, tested, and finally deployed
   - often  value stream has many steps with hundreds of people involved
   - documenting is complex and may require a  multi-day workshop  with all key
     constituents present
   - goal not to document every step, but to sufficiently understand the areas
     in value stream jeopardizing our goals of fast flow, short lead times,
     and reliable customer outcomes
   - focus on:
     - places where work must wait weeks or even months, such as getting
       production-like  environments, change approval processes, or security
       review processes
     - places where significant rework is  generated or received
   - first pass should only have high  level process blocks, maybe five to fiften
   - each process block should include lead time and process time, as well as
     the percent C/A (complete/accurate) as measured by downstream consumers
   - identify which metric should be improved then construct future value map as
     target condition
   - brainstorm hypotheses and  countermeasures to achieve  desired improvement,
     perform experiments  to test hypotheses, and interpret results to determine
     whether the hypotheses were correct
** Creating a dedicated transformation team
   - how to have change while still  maintaining day to day  operation
     - assign members of dedicated transformation  team to  devops
       transformation efforts solely
     - select team members who are generalists with wide array of skills
     - select team members who have longstanding and mutually respectful
       relationships with rest of organization
     - create separte  physical space for dedicated team if  possible
** Agree on a shared goal
   - achievement of the goal  should create obvious value for the organization
   - define a measurable goal with clearly defined deadline, between  six
     months and two years
   - don't have too many of these going on at the same time
   - examples
     - reduce the percentage of the budget spent on product  support  and
       unplanned  work by 50%
     - ensure lead time from code check-in to production release is  one week or
       less for 95% of changes
     - ensure releases can always  be  performed during normal business hours
       with zero downtime
     - integrate all the required information security controls into the
       deployment pipeline to pass all  required compliance  requirements
   - have several  week iterations to progress by steps towards larger goal
** Keep our improvement planning horizons short
   - flexibility  and ability to reprioritize and replan quickly
   - decrease delay between work expended and improvement realized, which
     strengthens feedback  loop
   - faster learning generated from first iteration
   - reduction  in activation energy to get improvements
   - quicker realization of improvements that make meaningful differences in
     daily work
   - less risk that our project is killed before we can generate any
     demonstrable outcomes
** Reserve 20% of cycles for non-functional requirements and reducing technical debt
*** Case Study: Operation InVersion at LinkedIn (2011)
    - initial state
      - monolithic Java application (Leo) that served every page through servlets and
        managed jdbc connections to various oracle databases
      - in 2010, most new development was in new services outside of Leo but Leo
        was only deployed every two weeks
      - Leo often went down in production, difficult to troubleshoot and recover,
        difficult to release new code
      - "when LinkedIn would try to  add a bunch of new things at  once, the site
        would crumble into a broken mess, requiring engineers to work long into
        the night and fix the problems"
    - solution
      - decided to stop work on new features and focus on fixing infrastructure:
        Operation InVersion
      - developed suite of software and tools to help develop code for site
      - engineers could develop service, inspect it with automated tests, and
        launch it into production
      - major upgrades now hapen three times a day
*** Increase the visibility of work
    - need up to date data
    - constantly revise what we measure to make sure it's helping us understand
      progress toward current target conditions
** Use tools to reinforce desired behavior
   - tooling should reinforce that development and operations have shared goals,
     shared backlog of work, in common work system using shared vocabulary so
     that work can be prioritized globally
   - chat rooms reinforce shared goals by allowing fast flow of information
     - have to monitor  so that there  aren't too many interruptions
* How to design our organization and architecture with Conway's law in mind
  - Conway's law "organizations which design systems are constrained to produce
    designs which are copies of the communication structures  of these
    organizations...The larger an organization is, the less  flexibility it has
    and the more pronounced the phenomenon"
  - example:  Etsy Sprouter
    - problem
      - sprouter hid the database from the application
      - required devs to ask dbas for any new stored procedures, had to wade
        through bereaucracy
      - conway's law, sprouter arose because two separate teams, dev and dbas
      - but sprouter, which was intended to isolate the teams from each other,
        became another layer that had to be maintained anytime anything in either
        team changed
    - solution
      - moved all business logic from database into application, removing need
        for sprouter
      - by eliminating sprouter, they eliminated cross team dependencies, number
        of handoffs, and increased speed/success of production deployments
** Organizational Archetypes
   - functional
     - optimize for expertise, division of labor, or reducing cost
     - centralize expertise
     - tall hierarchical structures
   - matrix
     - attempt to combine functional and market
     - complicated organizational structure, such as people reporting to two
       different managers
   - market
     - optimize for responding quickly to customer needs
     - flat structure, composed of multiple cross functional disciplines, often
       leading to potential redundancies across the company
     - devops organizations tend to use this structure
** Problems often caused by overly functional orientation ("optmizing for cost")
   - slow lead time because of many handoffs between functional groups (devs,
     dbas, SAs, etc)
   - person doing the work has little awareness of how it contributes to the
     value stream ("Someone told me to configure this server"), creating a
     creativity and motivation vacuum
   - problem increased when each operations functional group has to serve
     multiple development teams who all compete for their cycles
   - development teams have to escalate issues to managers who can globally
     prioritize and this cascades down to the functional groups
** Enable market-oriented teams ("optimizing for speed")
   - market oriented teams responsible for development, testing, securing,
     deploying, and supporting service in production
   - enables each team to deliver independently from other teams
** Making functional orientation work
   - work is prioritized transparently and sufficient slack in system to allow
     high-priority work to be completed quickly
   - enabled by automated self-service platforms
   - Mike Rother wrote in Toyota Kata "As tempting as it seems, one cannot
     reorganize your way to continuous improvement and adaptiveness. What is
     decisive is not the form of the organization, but how people act and react.
     The roots of Toyota's success lie not in its organizational structures, but
     in developing capability and habits in its people. It surprises many, in
     fact, to find that Toyota is largely organized in a traditional,
     functional-department style"
** Testing, Operations, and Security as everyone's job, every day
   - facebook was struggling with issues in production
   - one solution was to have every engineer, engineering manager, and architect
     rotate on-call duty for the services they built
** Enable every team member to be a generalist
   - if everyone is only a specialist, too many handoffs
   - specialize but also generalize
   - invest in the people we have by encouraging them to learn and grow
   - yes they may be more expensive than specialists but they are also much more productive
** Fund not projects, but services and products
   - don't move developers to another project after initial development is done
   - have developers stick around on same project for the brownfield phase so
     they learn to live with the errors they've made and how to make the product
     more resilient/useful in production
** Design team boundaries in accordance with Conway's law
   - separate testers and devs can create communication issues
** Create loosely coupled architectures to enable developer productivity and safety
   - services that can be updated independently of each other
   - no shared databases between services
   - bounded contexts between services enforce encapsulation - devs should only
     need to know other services' api
** Keep team sizes small (the "two-pizza team" rule)
   - usually about five to ten people
   - important effects:
     - clear, shared understanding of the system
     - limit growth rate of product or service. Helps ensure that people
       continue to understand service
     - decentralizes power and enables autonomy. Team lead works with executive
       team to determine key metric by which team can measure its progress
     - small teams allows more leadership experience without catastrophic results
*** Case study: API enablement at Target (2015)
    - problem
      - took ten different teams to provision server
      - when things broke, stopped making changes out of fear
      - core data locked up in legacy mainframes
      - multiple sources of truth, with different teams, data structures, priorities
      - long amounts of time spent on integration between various systems, with
        lots of manual testing
    - solution
      - API Enablement Team
      - internal team, no contractors, to ensure low lag time and top
        engineering skill
      - took over ops responsibilities
      - started using cassandra and kafka for scaling (asked permission, was
        refused, and did it anyway because they needed it)
      - in following two years, enabled fifty-three new business capabilities
      - eighty deployments per week
      - digital sales increased 42% during 2014
      - 280k in-store pickup orders in 2015 black friday
* How to get great outcomes by integrating operations into the daily work of development
** Create shared services to increase developer productivity
   - most of these services should be self service, without requiring submitting
     a ticket, so that operations doesn't become a bottleneck
   - "without these self-service operations platforms, the cloud is just
     Expensive Hosting 2.0"
   - examples
     - shared version control repository with pre-blessed security libraries
     - deployment pipeline that automatically runs code quality and security
       scanning tools
     - deploys into known, good environments that already have production
       monitoring tools installed
   - platform team can educate development teams and spread best practices and
     tools across teams so that it's easier for developers to move between teams
     without always having to relearn a new set of tools
** Embed ops engineers into our service teams
   - product teams often have the budget to fund hiring these ops engineers
   - these ops engineers are driven by team priorities, not overall priorities
     and internal ops concerns
   - efficient way to cross-train operations knowledge and expertise into
     service team
** Assign an ops liason to each service team
   - liason responsible for understanding:
     - what is the new product functionality and why we're building it
     - how it works as it pertains to operability, scalability, and observability
     - how to monitor and collect metrics to ensure the progress, success, or
       failure of the functionality
     - any departures from previous architectures and patterns, and the
       justifications for them
     - any extra needs for infrastructure and how usage will affect
       infrastructure capacity
     - feature launch plans
** Integrate ops into dev rituals
** Invite ops to our dev standups
** Invite ops to our dev retrospectives
** Make relevant ops work visible on shared kanban boards
   - put operations work relevant to product delivery on kanban board
* Part III: The First Way - The technical practices of flow
** Create the foundations of our deployment pipeline
   - production like environments at every stage of value stream
   - created in automated, on demand way
** Enable on demand creation of dev, test, and production environments
   - use automation for any or all of the following
     - copying a virtualized environment (eg vmware image, running a vagrant
       script, booting an amazon machine image file in ec2)
     - building an automated environment creation process that starts from "bare
       metal" (eg PXE install from a baseline image)
     - using "infrastructure as code" configuration management tools (eg puppet,
       chef, ansible salt, cfeengine, etc)
     - using automated operating system configuration tools (eg solaris
       jumpstart, red hat kickstart, debian preseed)
     - assembling an environment from a set of virtual images or containers (eg
       vagrant, docker)
     - spinning up a new environment in a public cloud (eg amazon web services,
       google app engine, microsoft azure), private cloud, or other paas (platform
       as a service, such as openstack or cloud foundry, etc)
   - providing developers environment they fully control allows them to test
     changes to environment, get feedback quickly, etc
** Create our single repository of truth for the entire system
   - put everything, including QA, operations, infosec, etc into revision control
   - check in the following assets to our shared version control repository
     - all application code and dependencies (eg libraries, static content, etc)
     - any script used to create database schemas, application reference data, etc
     - all environment creation tools and artifacts (eg vmware or ami images,
       puppet or chef recipes, etc)
     - any file used to create containers (eg docker or rocket definition or
       composition files)
     - all supporting automated tests and any manual test scripts
     - any script that supports code packaging, deployment, database migration,
       and environment provisioning
     - all project artifacts (eg requirements docs, deployment procedures,
       release notes, etc)
     - all cloud configuration files (eg aws cloudformation templates, microsoft
       azure stack dsc files, openstack heat)
     - any other script or configuration information required to create
       infrastructure that supports multiple services (eg enterprise service
       buses, database management systems, dns zone files, configuration rules
       for firewalls, and other networking devices)
     - must be able to recreate pre-production and build processes as well as
       production environments (ie tools, compilers, testing)
