#+STARTUP entitiespretty

* Prerequisites
- math
  - discrete math
  - proofs by induction
  - Mathematics for Computer Science by Eric Lehman and Tom Layden
- programming
* Merge Sort
- example of divide and conquer
  - break problem into smaller problems then combine the smaller solutions 
* Big Oh
** Omega Notation
- T(n)=Omega(f(n)) if and only if there exists constants C,n0 such that T(n) >= C*f(n) for all n >= n0
** Little-oh Notation
- strictly less than vs big oh which is equal to or less than
** Examples
*** Claim: 2^n+10 = O(2^n)
- Proof: need to pick constants such that
  - (*) 2^n+10 <= c*2^n for all n >= n_0
  - Note: 2^n+10 = 2^10 * 2^n = 1024 * 2^n
  - so if we choose c=1024, n_0=1, then (*) holds
* Divide and Conquer Algorithms 
** Steps
- DIVIDE into smaller problems
- CONQUER via recursive calls
- COMBINE solutions of subproblems into one solution
** An example problem
*** Overview
- Input: array A containing the numbers 1,2,3,... in some arbitrary order
- Output: number of inversions. In other words, number of pairs (i, j) of array indices with i < j and A[i] > A[j].
*** Example 1
- (1, 3, 5, 2, 4, 6)
- Inversions: (3,2), (5,2), (5,4)
- Motivation: numerical similarity measure between two ranked lists (e.g. for "collaborative filtering" - propose other things to buy based on other
people with similar purchasing history)
*** solutions
**** brute force: double for loop O(n^2)
**** divide and conquer
- high level
  Count(array A, length n)
    if n=1 return 0
    else
      x=Count(1st half of A, n/2)
      y=Count(2nd half of A, n/2)
      z=CountSplitInv(A,n)
    return x+y+z
- implementation
  - key idea: have recursive calls both count inversions *and* sort (i.e. piggyback on mergesort)
  - motivation: merge subroutine naturally uncovers split inversions
  - example
    - merging [1,3,5] and [2,4,6]
    - when 2 copied to output, discover the split inversions (3,2) and (5,2)
    - when 4 copied to outpu, discover the split inversion (5,4)
  - claim: the split inversions involving an element y of the 2nd array C are precisely the numbers left in the 1st array B when y is copied to the output D
- solution:
  - claim: the split inversions involving and element y in the second array C are precisely the numbers left in the first array B when y is copied to the output D
  - proof:
    - let x be an element of the array B
    - if x is copied to output D before y, then x < y => no inversion involving x and y
    - if y is copied to output D before x, then y < x => x and y are a split inversion
* Strassen's Subcubic Matrix Multiplication Algorithm 
** Matrix Multiplication
*** Definition
- x[][] * y[][] = z[][]
- z_ij = (ith row of x) * (jth column of y) = sum of k=1 to n x_ik * y_jk
*** Example
| a | b | * | e | f | = | ae + bg | af + bh |
| c | d |   | g | h |   | ce + dg | cf + dh |
*** Applying Divide and Conquer
- Idea: 
  write x =
  | a | b |
  | c | d |
  and y =
  | e | f |
  | g | h |
[where A through H are all n/2 * n/2 matrices]
- Then: 
x*y = 
| ae + bg | af + bh |
| ce + dg | cf + dh |
*** Recursive Algorithm #1
- recursively compute the 8 necessary products
- do the necessary additions (O(n^2) time)
- *Fact: runtime is O(n^3)*
*** Strassen's Algorithm (1969)
- recursively compute only 7 (cleverly chosen) products
- do the necessary (clever) additions and subtractions (still O(n^2) time)
- *Fact: better than cubic time!*
**** The Details
- the seven products
  - P1=A(F-H)
  - P2=(A+B)H
  - P3=(C+D)E
  - P4=D(G-E)
  - P5=(A+D)(E+H)
  - P6=(B-D)(G+H)
  - P7=(A-C)(E+F)
- *claim*: x*y=
| p5+p4-p2+p6 | p1+p2       |
|-------------+-------------|
| p3+p4       | p1+p5-p3-p7 |
**** How could you figure this out?
[[http://softwareengineering.stackexchange.com/questions/199627/how-did-strassen-come-up-with-his-matrix-multiplication-method][how did strassen come up with his matrix multiplication method]]
* Algorithm for Closest Pair 
** Problem
- Input: a set p = {p_1,...,p_n} of n points in the plane (R2)
- Notation: d(p_i, p_j) = Euclidean distance
  So if p_i=(x_i, y_i) and p_j=(x_j,y_j), d(p_i, p_j) = square root of (x_i -
  y_i)^2 + (x_j - y_j)^2
- Output: a pair p*, q* in the set p of distinct points that minimi
- Assumption: all points have distinct x-coordinates, distinct y-coordinates
- Brute force search: takes Theta(n^2) time
- 1-dimension version of closest pair: all points lie on a line
  - sort points (O(n log n) time)
  - return closest pair of adjacent points (O(n) time)
- Goal: O(n log n) time algorithm for 2-D version
*** High level approach
    - make copies of points sorted by x-coordinate (Px) and by y-coordinate (Py)
      - sorting is sort of a "for free" data transformation in algorithms
    - use divide and conquer
*** ClosestPair(Px, Py)
    1. let Q = left half of P, R = right half of P
       - base case, once you have only 2 or 3 points, brute search for closest points
       - form Qx, Qy, Rx, Ry (takes O(n) time)
    2. (p_1, q_1) = ClosestPair(Qx, Qy)
    3. (p_2, q_2) = ClosestPair(Rx, Ry)
    4. let delta = min {d(p1, q1), d(p2, q2)}
    5. (p3, q3) = ClosestSplitPair(Px, Py, delta)
    7. return best of (p1, q1), (p2, q2), (p3, q3)
    8. KEY IDEA: only need to bother computing the closest split pair in "unlucky
       case" where its distance is less than d(p1, q1) and d(p2, q2)
*** ClosestSplitPair(Px, Py, delta)
    - let x| = biggest x - coordinate in left of P *(O(1) time)*
    - let Sy = points of P with x-coordinate in 
      [x| -delta, x| +delta], sorted by y-coordinate *(O(n) time)*
    - Initialize best = delta, bestpair = null
    - For i = 1 to |Sy| - 1
      - For j=1 to min{7, |Sy| - i}
        - let p_ij = ith, (i+j)th points of Sy
        - If d(p, q) < best
          - bestpair = (p,q), best = d(p,q)
*** Correctness Claim
    - Claim: let p in Q, q in R be a split pair with d(p, q) < delta
    - Then:
      - A: p and q are members of Sy
      - B: p and q are at most 7 positions apart in Sy
    - Corollary 1: If the closest pair of P is a split pair, then ClosestSplitPair
      finds it
    - Corollary 2: ClosestPair is correct, and runs in O(n log n) time
*** Proof of Correctness Claim (A)
    - let p = (x1, y1) in Q, q=(x2, y2) in R, d(p, q) < delta
      - note: since d(p, q) < delta, |x1 - x2| < delta and |y1 - y2| < delta
    - Proof: [p and q are members of Sy, ie x1, x2 in [x| - delta, x| + delta]]
      - note: p in Q => x1 <= x| and q in R => x2 >= x|
      - => x1, x2 in [x| - delta, x| + delta]
*** Proof of Correctness Claim (B)
    - B: p=(x1, y1) and q=(x2,y2) are at most 7 positions apart in Sy
    - key picture: draw delta/2 * delta/2 boxes with center x| and bottom min {y1,y2}
    - lemma 1: all points of Sy with y-coordinate between those of p and q, inclusive,
      lie in one of these 8 boxes
      - proof: first, recall y-coordinates of p,q differ by < delta
      - second, by definition of Sy, all have x-coordinates between x| - delta and
        x| + delta
    - lemma 2: at most one point of p in each box
      - proof: by contradiction. Suppose a,b lie in the same box. Then:
        - (i) a,b are either both in Q or both in R
        - (ii) d(a, b) <= delta/2 * square root of 2 < delta
        - But (i) and (ii) contradict the definition of delta (as smallest distance between
          pair of points in Q or in R)
*** Final wrap-up
    - lemmas 1 and 2 => at most 8 points in this picture (including p and q)
    - => positions of p,q in Sy differ by at most 7
* Master Method
  - Motivation: potentially useful algorithmic ideas often need mathematical analysis to 
   evaluate
  - Recall: grade-school multiplication algorithm uses theta(n^2) operations to
    multiply two n-digit numbers
** A recursive algorithm
   - recursive approach: write x=10^(n/2)a + b y=10^(n/2)c + d
     [where a,b,c,d are n/2 digit numbers]
   - So: x*y=10^n(ac) + 10^(n/2)(ad+bc) + bd (*)
   - Algorithm #1: recursively compute ac, ad, bc, bd then compute (*) in obvious
     way
     - T(n) = maximum number of operations this algorithm needs to multiply two 
       n-digit numbers
     - recurrence: express T(n) in terms of running time of recursive calls
     - base case: T(1) <= a constant
     - for all n > 1: T(n) <= 4T(n/2) + O(n)
** A better recursive algorithm
   - algorithm #2 (Gauss): recursively compute (1) ac, (2) bd, (3) (a+b)(c+d)
     [recall ad+bc=(3) - (1) - (2)]
   - new recurrence:
     - base case: T(1) <= a constant
     - for all n > 1: T(n) <= 3T(n/2) + O(n)
** The Master Method
   - cool feature: a "black box" for solving recurrences
   - assumption: all subproblems have equal size
*** recurrence format
    - base case: T(n) <= a constant for all sufficiently small n
    - for all larger n:
      - T(n) <= aT(n/b) + O(n^d)
      - where
        - a = number of recursive calls (>= 1)
        - b = input size shrinkage factor (> 1)
        - d = exponent in running time of "combine step" (>=0)
        - [a,b,d independent of n]
*** definition
    - T(n) = 
      - O(n^d log n) if a = b^d
      - O(n^d) if a < b^d
      - O(n^log_b(a)) if a > b^d
*** examples
**** example 1: Merge Sort
     - a = 2
     - b = 2
     - d = 1
     - a = 2 = b^d
     - => T(n) <= O(n^d log n) = O(n log n)
**** example 2: binary search
     - a = 1
     - b = 2
     - d = 0
     - a = b^d => T(n) = O(n^d log n) = O(log n)
**** example 3: integer multiplication
     - a = 4
     - b = 2
     - d = 1
     - b^d=2 < a
     - => T(n) = O(n^log_b(a)) = O(n^log_2(4)) = O(n^2)
**** example 4: gauss's integer multiplication
     - a = 3
     - b = 2
     - d = 1
     - b^d=2 < a
     - => T(n) = O(n^log_b(a)) = O(n^log_2(3))
**** example 5: strassen's matrix multiplication
     - a = 7
     - b = 2
     - d = 1
     - b^d=4 < a
     - => T(n) = O(n^log_2(7)) = O(n^2.81)
     - => beats the naive iterative algorithm O(n^3)
**** example 6: fictitious recurrence
     - T(n) <= 2T(n/2) + O(n^2)
     - a = 2
     - b = 2
     - d = 2
     - b^d=4 > a (case 2)
     - => T(n) = O(n^2)
** Proof
*** Preamble
    - assume: recurrence is (for some constant c)
      - (i) T(1) <= c
      - (ii) T(n) <= aT(n/b) + cn^d 
      - and n is a power of b (general case is similar, but more tedious)
    - Idea: generalize merge sort analysis (ie, use a recursion tree)
*** Work at a single recursion level
    - a^j = # of level-j subproblems
    - n/b^j = size of each level-j subproblem
    - c * (n/b^j)^d = work per level-j sub problem
    - <= a^j * c * (n/b^j)^d = cn^d * (a/b^d)^j
*** Total work
    - summing over all levels
      - total work <= cn^d * sum from j=0 to log_b(n) [a/b^d]^j = *
*** How to think about *
    - interpretation
      - a = rate of subproblem proliferation (RSP)
      - b^d = rate of work shrinkage per subproblem (RWS)
*** Intuition for the 3 Cases
    - upper bound for level j: cn^d * (a/b^d)^j
      - rsp = rws => same amount of work each level (like merge sort) [expect
        O(n^d log n)]
      - rsp < rws => less work each level => most work at the root [might expect O(n^d)]
      - rsp > rws => more work each level => most work at the leaves [might
        expect O(#leaves)]
*** The Story so far/Case 1
    - total work: <= cn^d * sum j=0 to log_b(n) do (a/b^d)^j = (*)
    - if a=b^d, then
      - (*) = cn^d (log_b(n) + 1)
      - = O(n^d log n)
*** Basic Sums Fact
    - For r!=1, we have 1 + r + r^2 + r^3 = (r^(k+1) -1)/(r - 1)
    - Proof: by induction
    - Upshot:
      - if r < 1 is constant, <= 1/(1-r) = a constant (independent of k) (ie
        first term of sum dominates)
      - if r>1 is constant, RHS is <= r^k * (1 + 1/(r-1)) (last term of sum dominates)
*** Case 2
    - total work:
      - if a < b^d [RSP < RWS] then a/b^d <= a constant (independent of n)
      - = O(n^d)
      - [total work dominated by top level]
*** Case 3
    - total work:
      - if a > b^d [RSP > RWS] then a/b^d > 1 and a/b^d <= constant * largest term
      - = O(n^d * (a/b^d)^(log_b(n)))
      - note: b^(-d log_b(n)) = (b^log_b(n))-d = n^-d
      - so: (*) = O(a^log_b(n))
      - a^(log_b(n)) = # leaves in recursion tree
      - a^log_b(n) = n^log_b(a) since (log_b(n))(log_b(a)) = (log_b(a))(log_b(n))
* QuickSort
  - "greatest hit" algorithm
  - prevalent in practice
  - beautiful analysis
  - O(n log n) time "on average", works in place (i.e. minimal extra memory needed)
** Overview
*** The sorting problem
    - input: array of n numbers, unsorted
    - output: same numbers, sorted in increasing order
    - assume: all array entries distinct
    - exercise: extend quicksort to handle duplicate entries
*** Partitioning around a pivot
    - key idea: partition array around a pivot element
    - pick element of array [3] 8 2 5 1 4 7 6
    - rearrange array so that:
      - left of pivot => less than pivot
      - right of pivot => greater than pivot
      - 2 1 [3] 6 7 4 5 8
    - Note: puts pivot in its "rightful position"
*** Two cool facts about partition
    - linear (O(n)) time, no extra memory
    - reduces problem size
*** QuickSort: high-level description
    - Hoare circa 1961
    - QuickSort(array A, length n)
      - if n = 1 return
      - p = ChoosePivot(A, n)
      - partition A around p
      - recursively sort 2nd part
      - recursively sort 1st part
** Partitioning around a Pivot
*** The easy way out
    - Note: using O(n) extra memory, easy to partition around pivot in O(n) time
*** In-place implementation
    - *Assume*: pivot = 1st element of array (if not, swap pivot <-> 1st element
      as preprocessing step)
    - *high-level idea*: [p] [<p] [>p] [unpartitioned]
      - single scan through array
      - invariant: everything looked at so far is partitioned
*** Partition example
    - [p] [<p] i [>p] j [?]
    - [3] ij [8 2 5 1 4 7 6]
    - [3] i [8] j [2 5 1 4 7 6]
    - [3] [2] i [8] j [5 1 4 7 6]
    - [3] [2] i [8 5] j [1 4 7 6]
    - [3] [2 1] i [5 8] j [4 7 6]
    - [3] [2 1] i [5 8 4] j [7 6]
    - [3] [2 1] i [5 8 4 7] j [6]
    - [3] [2 1] i [5 8 4 7 6] j
    - [1 2] [3] [5 8 4 7 6]
*** Pseudocode for partition
    - partition(A, l, r) #input = A[l...r]
    - p=A[l]
    - i=l+1
    - for j=l+1 to r
      - if A[j] < p #if A[j]>p, do nothing
        - swap A[j] and A[i]
        - i=i+1
    - swap A[l] and A[i-1]
*** Running Time
    - running time = O(n), where n=r-l+1 is the  length of the input of the (sub)array
    - *reason*: O(1) work  per  array entry
    - also: clearly works in place (repeated swaps)
*** Correctness
    - *claim*: the for loop maintains  the invariant: [p] [<p] i [>p] j [?]
    - A{l+1},...,A[i-1] are all less than the pivot
    - A[i],...,A[j-1] are all greater than pivot
    - consequence: at end of for loop, have [p] [<p] i [>p] j => after final
      swap, array partitioned around pivot
** Correctness of Quicksort
*** Induction Review
    - let  P(n) = assertion parameterized  by positive  integers n
    - for us: P(n) is "quicksort correctly sorts every input array of length n"
    - how to prove P(n) for all n>= 1 by induction:
      - base case: directly prove that P(1) holds
      - inductive step: for  every  n>=2, prove that: if P(k) holds for  all
        k<n, then P(n) holds as well
*** correctness
    - P(n) = "quicksort correctly sorts every input array  of length n"
    - *claim*: P(n) holds for every n>=1 (no  matter how pivot is chosen)
    - proof by induction:
      - base case:  every  input array of length 1 is already sorted. Quicksort
        returns  the input array, which  is correct  (P(1) holds)
      - inductive step:  Fix n>=2. Fix some input array of length n.
        - Need to show: if P(k) holds for all k<n, then  P(n)  holds as well
        - recall: quicksort first partitions A around some pivot point  p.
        - note: pivot winds up in correct position. Let k_1,k_2=lengths of 1st,
          2nd parts of partitioned array (k_1, k_2 < n)
        - by inductive hypothesis: 1st, 2nd parts get sorted correctly by
          recursive calls (using P(k_1), P(k_2))
        - so: after recursive calls,  entire array  correctly sorted
** Choosing a good pivot
   - Q: running time of Quicksort?
   - A: depends on the  quality  of the pivot
   - if doing Quicksort on already sorted array  and choosing first element  as
     pivot, the  running time would be theta(n^2)
     - reason: [] [p] [>p]
     - runtime: >= n + (n-1) + (n-2)...+ 1
     - 1st n/2 terms all  at  least n/2
   - if doing Quicksort and every recursive  call chooses the median element,
     the runtime would be theta(n log n)
     - reason: let T(n) = running time  on arrays of size n
     - then: T(n) <= 2T(n/2) + O(n) => T(n) = O(n log n) [like MergeSort]
*** Random pivots
    - key question: how to choose pivots?
    - *big idea*: random pivots
    - that is: in every recursive call, choose the pivot randomly  (each element
      equally likely)
    - hope: a random  pivot is "pretty good often enough"
    - intuition:
      - if always get a 25-75 split, good enough for  O(n log n)
        running time [non-trivial exercise: prove via recursion tree]
      - half of elements give  a 25-75  split or better
      - Q: does this really work?
*** Average running time of quicksort
    - quicksort theorem: for every  input array of a length n, the average
      running time of quicksort with random pivots  is O(n log n)
    - note: holds for every input  (no assumptions on the data)
    - recall  our guiding  principles
    - "average" is over random choices made by the algorithm (ie pivot choices)
** Probability Review
*** Sample spaces
    - sample space omega = "all possible outcomes" [in algorithms, omega is
     usually  finite]
    - also: each  outcome i element of omega has a probability  p(i)  >= 0
    - constraint: sum_(elements of omegaa) p(i) =  1
    - example #1: rolling 2  dice. omega = {(1,1),(2,1),(3,1),...,(5,6),(6,6)}
      and p(i)=1/36 for all i in omega
    - example #2: choosing a random pivot in outer quicksort call. omega =
      {1,2,...,n} (index of  pivot) and p(i)=1/n for all i in omega
*** Events
    - note: C= means subset or equal to
    - an event is a subset S C= omega
    - the probability of an event S is sum_(i in S) p(i)
    - the probability of the  event where the  sum  of the two  dice  is 7 is  1/6
      - S = {(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)}
      - Pr[S]=6/36=1/6
    - the probability  for the event where the chosen quicksort pivot gives a
      25-75 split or better is 1/2
      - S={(n/4+1)th smallest element,...,(3n/4)th smallest element}
      - Pr[S]=(n/2)/n=1/2
*** Random  variables
    - a random variable X is a real-valued function
      - X: omega -> |R
    - example 1: sum of the two dice
    - example 2: size of subarray passed to the 1st recursive call
**** Expectation
     - let X: omega -> |R be a random variable
     - the expectation E[X] of X = average value of  X
       - = sum_(i in omega) X(i) * p(i)
     - the expectation of the sum of two dice is  7
**** Linearity of Expectation
     - *claim*: let x_1,...,X_n be random variables  defined on  omega.  Then:
       - E[sum_(i=1)^n X_j] = sum_(i=1)^n E[X_j]
       - crucially: holds even when X_j's are  not independent
     - example #1:
       - if  X_1, X_2 = the two dice, then E[X_j] = 1/6(1+2+3+4+5+6) = 3.5
       - by linear expectation: E[X_1]+E[X_2]=E[k_1]+E[X_2]=3.5+3.5=7
**** Linearity of Expectation (Proof)
     - sum_(j=1)^n E[X_j] = sum_(j=1)^n sum_(i in set omega) X_j(i)p(i)
     - = sum_(i in set omega) sum_(j=1)^n X_j(i)p(i)
     - = sum_(i in set omega) p(i) (sum_(j=1)^n X_j(i))
     - = E[sum_(j=1)^n X_j]
**** Example: Load Balancing
     - Problem: need to  assign n processes to n servers
     - proposed  solution: assign each process to  a random server
     - question: what is expected number of processes assigned to a server?
     - sample space  omega = all n^n assignments of processes to servers, each
       equally likely
     - let y=total number  of processes  assigned to first server
     - goal: compute E[y]
     - let x_j=
       - 1 if  jth process assigned to first server
       - 0 otherwise
       - binary random variables are called indicator random variables
       - note: y=sum_(j=1)^n X_j
     - we have
       - E[y]=E[sum_(j=1)^n X(j)] #since y=sum_(j=1)^n X_j)
       - = sum_(j=1)^n E[X_j] #lin exp
       - = sum_(j=1)^n (Pr[X_j=0]*0  + Pr[X_j=1]*1)
       - = sum_(j=1)^n (1/n)
       - = 1
*** Conditional probability
    - let x,y C= omega  be  events
    - then Pr[x|y] ("x given y") = Pr[x n y]/Pr[y]
    - given that the sum of  the  two dice is 7, the probability that at least
      one die  is a 1 is 1/3
      - x =  at least one die is a 1
      - y = sum of two dice = 7
        - = {(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)}
        - => x n y  = {(1,6), (6,1)}
*** Independence
**** Independence (of Events)
     - *definition*: events x,y c= omega are independent if (and only if) Pr[x n
       y] = Pr[x] * Pr[y]
     - Pr[x|y] = Pr[x]
     - Pr[y|x] = Pr[y]
     - warning: can  be a  very subtle  concept (intuition is often incorrect!)
**** Independence (of  random variables)
     - *definition*: random variables A,B (both defined on omega) are
       independent <=> the events Pr[A=a], Pr[B=b]  are independent for  all
       a,b. (<=>  Pr[A=a and B=b] = Pr[A=a] * Pr[B=b])
     - *claim*: if  A,B are independent,  then E[A*B] = E[A] * E[B]
     - *proof*: E[A*B] =  sum_(a,b) (a*b) * Pr(A=a  and B=b)
     - = sum_(a,b) (a*b) * Pr[A=a] * Pr[B=b] #since A,B independent
     - = sum_a a *  Pr[A=a](sum_b b * Pr[B=b])
**** Example
     - let x_1, x_2 in set {0,1} be random, and x_3 = x_1 xor x_2
     - formally: omega = {000,  101, 011, 110} each equally likely
     - *claim*: x_1 and x_3  are independent random variables
     - *claim*: x_1*x_3 and  x_2 are not independent random variables
     - *proof*: suffices to show that E[x_1*x_3*x_2] != E[x_1,x_3]*E[x_2]
       - E[x_1*x_3*x_2] = 0
       - E[x_1*x_3] = 1/4
       - E[x_2] = 1/2
       - 1/2 * 1/4 = 1/8
       - 0 != 1/8
** Analysis 1: a decomposition principle
*** Necessary background
    - assumption: you know and remember  (finite) sample  spaces, random
      variables, expectation,  linearity of expectation. For review:
      - probability review 1 (video)
      - lehman-leighton notes (free pdf)
      - wikibook on discrete probability
*** Preliminaries
    - fix input array A of length n
    - *sample space omega* = all possible outcomes of random choices in quicksort
      (ie pivot sequences)
    - *key random variable*: for p in set omega, C(p) = # of comparisons between
      two input elements made by quicksort (given random choices p)
    - *lemma*: running time of quicksort dominated by comparisons
      - there is a constant c such that for all p in omega RT(p) <= c * C(p)
    - *remaining goal*: E[C] = O(n log n)
*** Building blocks
    - note: can't apply master method [random, unbalanced subproblems]
    - A = fixed input array
    - notation: z_i=ith smallest element of A
    - for r in omega, indies i<j, let
      - X_ij(p) = # of times z_i,z_j get compared in quicksort with pivot
        sequence r
    - for two fixed elements in array, the number of times these two elements
      can be compared during the execution of quicksort is 0 or 1
      - can't be compared twice
        - two elements compared only when one is the pivot, which is excluded
          from future recursive calls
        - each x_ij is an indicator (ie 0-1) random variable
*** A decomposition approach
    - so: C(p) = # of comparisons between input elements
      - x_ij(p) = # of comparisons between z_i and z_j
    - thus: for all p, C(p)=sum_(i=1)^(n-1) sum_(j=i+1)^n x_ij(p)
    - by linearity of expectation: E[C] = sum_(i=1)^(n-1) sum_(j=i+1)^n E[x_ij]
    - since E[x_ij] = 0 * Pr[x_ij=0] + 1 * Pr[x_ij=1] = Pr[x_ij=1]
    - thus E[C] = sum_(i=1)^(n-1) sum_(j=i+1)^n Pr[z_i, z_j get compared] (*)
*** A general decomposition principle
    - identify random variable Y that you really care about
    - express Y as sum of indicator random variables
      - Y = sum_(p=1)^m x_p
    - apply linearity of expectation:
      - E[Y] = sum_(p=1)^m Pr[x_p=1]
** Analysis 2: the key insight
*** The story so far
    - C(p) = # of comparisons quicksort makes with pivots p
    - x_ij(p) = # of times z_i and z_j get compared (ith,jth smallest entries in array)
    - *key claim*: for all i < j, Pr[z_i,z_j get compared] = 2/(j-1+1)
*** Proof of key claim
    - fix z_i,z_j with i < j
    - consider the set z_i,z_i+1,...,z_j-1,z_j
    - *inductively*: as long as none of these are chosen as a pivot, all are
      passed to the same recursive call
    - consider the first among z_i, z_i+1,...,z_j-1,z_j that gets chosen as a pivot
      - if z_i or z_j get chosen first, then z_i and z_j get compared
      - if one of z_i+1,...,z_j-1 gets chosen first, then z_i and z_j are never
        compared [split into different recursive calls]
    - note: since pivots always chosen uniformly at random, each of
      z_i,z_i+1,...,z_j-1,z_j is equally likely to be the first
    - => Pr[z_i,z_j get compared] = 2/(j-i+1)
    - so: E[C] = sum_(i=1)^(n-1) sum_(j=i+1)^n 2/(j-i+1)
** Analysis 3: final calculations
*** The story so far
    - E[C] = 2 * sum_(i=1)^(n-1) sum_(j=i+1)^n 1/(j-i+1) # O(n^2) terms
      - how big can inner sum be?
    - note: for each i, the inner sum is
      - sum_(j=i+1)^n 1/(j-i+1) = 1/2 + 1/3 + 1/4 + ...
    - so: E[C] <= 2 * n * sum_(k=2)^n 1/k
      - claim: this is <= n log n
*** Completing the proof
    - sum_(k=2)^n 1/k <= integral_1^n 1/x d*x = ln * 
* Linear time selection 
** Randomized Selection Algorithm
*** The problem
    - input: array A with n distinct numbers and a number i E {1,2,...,n}
    - output: ith order statistic (ie ith smallest element of input array)
      - 10 8 2 4 -> 8 is 3rd order statistic
    - example: median
      - i = (n+1)/2 for n odd
      - i = n/2 for n even
*** Reduction to sorting
    - n log n algorithm
      - apply mergesort
      - return ith element of sorted array
    - fact: can't sort any faster [see optional video]
    - next: O(n) time (randomized) by modifying QuickSort
    - Suppose we are looking for the 5th order statistic in an input array of
      length 10. We partition the array, and the pivot winds up in the third
      position of the partitioned array. We should search for the 2nd order
      statistic on the right side of the pivot.
*** Randomized selection
    - RSelect(array A, length n, order statistic i)
      - if n=1 return A[1]
      - choose pivot p from A uniformly at random
      - partition A around p into 1st and 2nd part [1st part] pivot [2nd part]
        - let j=new index of p
      - if j=i return p
      - if j>i return RSelect(1st part of A, j-1, i)
      - if j<i return RSelect(2nd part of A, n-j, i-j)
*** Properties of RSelect
    - claim: RSelect is correct (guaranteed to output ith order statistic)
    - proof: by induction
    - running time: depends on "quality" of the chosen pivots
    - the worst case running time is O(n^2)
      - suppose i=n/2
      - suppose choose pivot=minimum every time
      - => omega(n) time in each of omega(n) recursive
*** Running time of RSelect?
    - RunningTime: depends on which pivots get chosen
      - could be as bad as theta(n^2)
    - key: find pivot giving "balanced" split
    - best pivot: the median (but this is circular)
    - => would get recurrence T(n) <= T(n/2) + O(n)
      - => T(n) = O(n) (case 2 of master method)
    - hope: random pivot is "pretty good" "often enough"
    - RSelect Theorem: for every input array of length n, the average running
      time of RSelect is O(n)
    - holds for every iput (no assumptions on data)
    - "average" is over random pivot choices made by the algorithm
** Randomized Selection - Analysis
*** Proof 1: Tracking progress via phases
    - note: RSelect uses <= cn operations outside of the recursive call (for
      some constant c > 0) (from partitioning)
    - notation: RSelect is in phase j if current array size between
      (3/4)^(j+1) * n and (3/4)^j * n
      - x_j = number of recursive calls during phase j
    - note: running time of RSelect <= sum_(phases j) x_j * c * (3/4)^j*n
      - 3/4^j*n <= array size during phase j
      - c = work per phase of subproblem
      - x_j = # of phase j subproblems
*** Proof II: reduction to coin flipping
    - x_j = # of recursive calls during phase j
    - note: if RSelect chooses a pivot giving a 25-75 split or better then
      current phase ends! (new subarray length at most 75% of old length)
    - recall: probability of 25-75 split or better is 50%
    - so: E[x_j] <= expected number of times you need to flip a fair coin to get
      one "heads" (heads=good pivot, tails=bad pivot)
*** Proof III: coin flipping analysis
    - let N = number of coin flips until you get heads (a "geometric random variable")
    - note: E[N] = 1 + 1/2*E[N]
      - 1 = 1st coin flip
      - 1/2 = probability of tails
      - E[N] = # of further coin flips needed in this case
    - solution: E[N] = 2 (recall E[x_j] <= E[N])
*** Putting it all together
    - expected running time of RSelect <= E[cn sum_(phase j) (3/4)^j * x_j]
      - = cn sum_(phase j) (3/4)^j E[x_j]
      - E[x_j] <= E[# of coin flips n] = 2
      - <= 2 cn sum_(phase j) (3/4)^j
      - sum_(phase j) (3/4)^j <= 1/(1 - 3/4) = 4 (geometric sum)
      - <= 8cn = O(n)
** Deterministic Selection Algorithm
*** Guaranteeing a good pivot
    - recall: "best" pivot = the median! (seems circular!)
    - goal: find pivot guaranteed to be pretty good
    - key idea: use "median of medians"!
*** A deterministic choose pivot
    - ChoosePivot(A, n)
    - logically break A into n/5 groups of size 5 each
    - sort each group (eg using MergeSort)
    - copy n/5 medians (ie middle element of each sorted group) into new array C
    - recursively compute median of C (!)
    - return this as pivot
*** the DSelect Algorithm
    - DSelect(array A, length n, order statistic i)
      - break A into groups of 5, sort each group -> theta(n)
      - C = the n/5 "middle elements" -> theta(n)
      - p = Select(C, n/5, n/10) (recursively computes median of C) -> T(n/5)
      - partition A around p -> theta(n)
      - if j=i return p
      - if j<i return Select(1st part of A, j-1, i)
      - else if j>i return Select(2nd part of A, n-j, i-j)
    - DSelect makes 2 recursive calls
*** Running time of DSelect
    - DSelectTheorem: for every input array of length n, DSelect runs in O(n) time
    - warning: not as good as RSelect in practice
      - worse constants
      - not in place (higher memory usage)
    - history:
      - from 1973
      - authors: blum, floyd, pratt, rivest, tarjan
    - the asymptotic running time of step 1 of the DSelect algorithm is theta(n)
      - sorting an array with 5 elements takes <= 120 operations
*** Rough recurrence
    - let T(n) = maximum runing time of DSelect on an input array of length n
    - there is a constant c>=1 such that:
      - T(1) = 1
      - T(n) <= cn + T(n/5) + T(?)
*** the key lemma
    - key lemma: 2nd recursive call (in the 6 or 7) guaranteed to be on an array
      of size <= 7/10 n (roughly)
    - upshot: can replace "?" by "7/10 * n"
    - rough proof: 
      - let k = n/5 = # of groups
      - let x_i = ith smallest of the k "middle elements"
      - so pivot = x_k/2
    - goal: >= 30 % of input array smaller than x_k/2, >= 30% is bigger
*** rough proof of key lemma
    - thought experiment: imagine we lay out elements of A in a 2-D grid:
    - columns = the groups of 5
    - rows = members of groups
    - key point: x_k/2 bigger than 3 out of 5 (60%) of the elements in about 50
      % of the groups
      - => bigger than 30% of A (similarly, smaller than 30% of A)
*** example
    - input: 7 2 17 12 13 8 20 4 6 3 19 1 9 5 16 10 15 18 14 11
    - after sorting groups of 5: 2 7 12 13 17 | 3 4 6 8 20 | 1 5 (9) 16 19 | 10 11
      14 15 18
    - 9 = pivot
    - table of sorted groups
      |----+-----+----+----|
      | 20 |  19 | 17 | 18 |
      |  8 |  16 | 13 | 15 |
      |  6 | (9) | 12 | 14 |
      |  4 |   5 |  7 | 11 |
      |  3 |   1 |  2 | 10 |
*** Rough recurrence (revisited)
    - T(1) = 1, T(n) <= cn + T(n/5) + T(7/10*n)
    - note: different-sized subproblems => can't use master method!
    - strategy: "hope and check"
    - hope: there is some constant a [independent of n] such that T(n) <= an for
      all n >= 1 (if true, then T(n) = O(n) and algorithm is linear-time)
*** Analysis of rough recurrence
    - claim: let a = 10c
      - then T(n) <= an for all n >= 1
    - proof: by induction on n
    - base case: T(1) = 1 <= a * 1 (since a >= 1)
    - inductive step: [n > 1] inductive hypothesis: T(k) <= ak for all k < n
      - we have T(n) <= cn + T(n/5) + T(7/10 * n)
      - <= cn + a(n/5) + a(7/10 * n)
      - = n(c + 9/10 a) = an
** Omega(n log n) lower bound for comparison based sorting
*** a sorting lower bound
    - theorem: every "comparison-based" sorting algorithm has worst-case running
      time beta(n log n) [assume deterministic, but lower bound extends to
      randomized algorithms]
    - comparison-based sort: accesses input array elements only via comparisons
      - = "general-purpose sorting method"
    - *examples*: MergeSort, QuickSort, HeapSort
    - *non-examples*: BucketSort (good for data from distributions), CountingSort
      (good for small integers), RadixSort
*** proof idea
    - fix a comparison-based sorting method and an array length n
    - => consider input arrays containing {1,2,3...,n} in some order
    - => n! such inputs
    - suppose algorithm always makes <= k comparisons to correctly sort these n! inputs
    - => across all n! possible inputs, algorithms exhibits <= 2k distinct
      executions (ie resolution of the comparisons)
    - by the pigeonhole principle: if 2k < n!, execute identically on two
      distinct inputs => must get one of them incorrect
    - so: since method is correct, 2k >= n!
    - >= (n/2)^(n/2)
    - => k >= n/2 log_2(n/2) n/2 = beta(n log n)
* Graphs and the Contraction Algorithm
** Graphs and minimum cuts
*** Goals for these lectures
    - further practice with randomized algorithms
      - in a new application domain (graphs)
    - introduction to graphs and graph algorithms
    - also: "only" 20 years ago!
*** Graphs
    - two ingredients
      - vertices aka nodes (V)
      - edges (E) = pairs of vertices
        - can be undirected (unordered pair) or directed (ordered pair) (aka arcs)
    - examples: road networks, the web, social networks, precedence constraints, etc
*** Cuts of graphs
    - definition: a cut of a graph (V, E) is a partition of V into two non-empty
      sets A and B
    - definition: the crossing edges of a cut (A, B) are those with:
      - one endpoint in each of (A,B) (undirected)
      - tail in A, head in B (directed)
    - there are roughly 2^n cuts in a graph with n vertices
*** the minimum cut problem
    - input: an undirected graph G=(V,E) [parallel edges allowed]
    - goal: compute a cut with fewest number of crossing edges (a min cut)
*** a few applications
    - identify network bottlenecks/weaknesses
    - community detection in social networks
    - image segmentation
      - input = graph of pixels
      - use edge weights [(u,v) has large weight => "expect" u,v to come from
        same object]
      - hope: repeated min cuts identifies the primary objects in picture
** Graph representations
   - consider an undirected graph that has n vertices, no parallel edges, and is
     connected (ie "in one piece"). The minimum number of edges is n - 1 and the
     maximum is n(n - 1)/2
*** Sparse vs dense graphs
    - let n = # of vertices, m = # of edges
    - in most (but not all) applications, m is omega(n) and O(n^2)
    - in a "sparse graph", m is O(n) or close to it
    - in a "dense graph", m is closer to theta(n^2)
*** Adjacency Matrix
    - represent G by a nxn 0-1 matrix A, where A_ij = 1 <=> G has an i-j edge
    - variants
      - A_ij = # of i-j edges (if parallel edges)
      - A_ij = weight of i-j edge (if any)
      - A_ij = {+1 if i->j
        - {-1 if i<-j
*** Adjacency Lists
    | ingredients                                | space        |
    |--------------------------------------------+--------------|
    | array (or list) of vertices                | theta(n)     |
    | array (or list) of edges                   | theta(m)     |
    | each edge points to its endpoints          | theta(m)     |
    | each vertex points to edges incident on it | theta(m)     |
    |--------------------------------------------+--------------|
    |                                            | theta(m + n) |
    
    - question: which is better?
    - answer: depends on graph density and operations needed
    - this course: focus on adjency lists
** Random Contraction Algorithm
   - compute minimum cut of a graph
   - due to karger, early 90s
   - while there are more than 2 vertices:
     - pick a remaining edge (u,v) uniformly at random
     - merge (or "contract") u and v into a single vertex
     - remove self-loops
   - return cut represented by 2 final vertices
*** Example
