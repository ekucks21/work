#+STARTUP entitiespretty

* Prerequisites
- math
  - discrete math
  - proofs by induction
  - Mathematics for Computer Science by Eric Lehman and Tom Layden
- programming
* Merge Sort
- example of divide and conquer
  - break problem into smaller problems then combine the smaller solutions 
* Big Oh
** Omega Notation
- T(n)=Omega(f(n)) if and only if there exists constants C,n0 such that T(n) >= C*f(n) for all n >= n0
** Little-oh Notation
- strictly less than vs big oh which is equal to or less than
** Examples
*** Claim: 2^n+10 = O(2^n)
- Proof: need to pick constants such that
  - (*) 2^n+10 <= c*2^n for all n >= n_0
  - Note: 2^n+10 = 2^10 * 2^n = 1024 * 2^n
  - so if we choose c=1024, n_0=1, then (*) holds
* Divide and Conquer Algorithms 
** Steps
- DIVIDE into smaller problems
- CONQUER via recursive calls
- COMBINE solutions of subproblems into one solution
** An example problem
*** Overview
- Input: array A containing the numbers 1,2,3,... in some arbitrary order
- Output: number of inversions. In other words, number of pairs (i, j) of array indices with i < j and A[i] > A[j].
*** Example 1
- (1, 3, 5, 2, 4, 6)
- Inversions: (3,2), (5,2), (5,4)
- Motivation: numerical similarity measure between two ranked lists (e.g. for "collaborative filtering" - propose other things to buy based on other
people with similar purchasing history)
*** solutions
**** brute force: double for loop O(n^2)
**** divide and conquer
- high level
  Count(array A, length n)
    if n=1 return 0
    else
      x=Count(1st half of A, n/2)
      y=Count(2nd half of A, n/2)
      z=CountSplitInv(A,n)
    return x+y+z
- implementation
  - key idea: have recursive calls both count inversions *and* sort (i.e. piggyback on mergesort)
  - motivation: merge subroutine naturally uncovers split inversions
  - example
    - merging [1,3,5] and [2,4,6]
    - when 2 copied to output, discover the split inversions (3,2) and (5,2)
    - when 4 copied to outpu, discover the split inversion (5,4)
  - claim: the split inversions involving an element y of the 2nd array C are precisely the numbers left in the 1st array B when y is copied to the output D
- solution:
  - claim: the split inversions involving and element y in the second array C are precisely the numbers left in the first array B when y is copied to the output D
  - proof:
    - let x be an element of the array B
    - if x is copied to output D before y, then x < y => no inversion involving x and y
    - if y is copied to output D before x, then y < x => x and y are a split inversion
* Strassen's Subcubic Matrix Multiplication Algorithm 
** Matrix Multiplication
*** Definition
- x[][] * y[][] = z[][]
- z_ij = (ith row of x) * (jth column of y) = sum of k=1 to n x_ik * y_jk
*** Example
| a | b | * | e | f | = | ae + bg | af + bh |
| c | d |   | g | h |   | ce + dg | cf + dh |
*** Applying Divide and Conquer
- Idea: 
  write x =
  | a | b |
  | c | d |
  and y =
  | e | f |
  | g | h |
[where A through H are all n/2 * n/2 matrices]
- Then: 
x*y = 
| ae + bg | af + bh |
| ce + dg | cf + dh |
*** Recursive Algorithm #1
- recursively compute the 8 necessary products
- do the necessary additions (O(n^2) time)
- *Fact: runtime is O(n^3)*
*** Strassen's Algorithm (1969)
- recursively compute only 7 (cleverly chosen) products
- do the necessary (clever) additions and subtractions (still O(n^2) time)
- *Fact: better than cubic time!*
**** The Details
- the seven products
  - P1=A(F-H)
  - P2=(A+B)H
  - P3=(C+D)E
  - P4=D(G-E)
  - P5=(A+D)(E+H)
  - P6=(B-D)(G+H)
  - P7=(A-C)(E+F)
- *claim*: x*y=
| p5+p4-p2+p6 | p1+p2       |
|-------------+-------------|
| p3+p4       | p1+p5-p3-p7 |
**** How could you figure this out?
[[http://softwareengineering.stackexchange.com/questions/199627/how-did-strassen-come-up-with-his-matrix-multiplication-method][how did strassen come up with his matrix multiplication method]]
* Algorithm for Closest Pair 
** Problem
- Input: a set p = {p_1,...,p_n} of n points in the plane (R2)
- Notation: d(p_i, p_j) = Euclidean distance
  So if p_i=(x_i, y_i) and p_j=(x_j,y_j), d(p_i, p_j) = square root of (x_i -
  y_i)^2 + (x_j - y_j)^2
- Output: a pair p*, q* in the set p of distinct points that minimi
- Assumption: all points have distinct x-coordinates, distinct y-coordinates
- Brute force search: takes Theta(n^2) time
- 1-dimension version of closest pair: all points lie on a line
  - sort points (O(n log n) time)
  - return closest pair of adjacent points (O(n) time)
- Goal: O(n log n) time algorithm for 2-D version
*** High level approach
    - make copies of points sorted by x-coordinate (Px) and by y-coordinate (Py)
      - sorting is sort of a "for free" data transformation in algorithms
    - use divide and conquer
*** ClosestPair(Px, Py)
    1. let Q = left half of P, R = right half of P
       - base case, once you have only 2 or 3 points, brute search for closest points
       - form Qx, Qy, Rx, Ry (takes O(n) time)
    2. (p_1, q_1) = ClosestPair(Qx, Qy)
    3. (p_2, q_2) = ClosestPair(Rx, Ry)
    4. let delta = min {d(p1, q1), d(p2, q2)}
    5. (p3, q3) = ClosestSplitPair(Px, Py, delta)
    7. return best of (p1, q1), (p2, q2), (p3, q3)
    8. KEY IDEA: only need to bother computing the closest split pair in "unlucky
       case" where its distance is less than d(p1, q1) and d(p2, q2)
*** ClosestSplitPair(Px, Py, delta)
    - let x| = biggest x - coordinate in left of P *(O(1) time)*
    - let Sy = points of P with x-coordinate in 
      [x| -delta, x| +delta], sorted by y-coordinate *(O(n) time)*
    - Initialize best = delta, bestpair = null
    - For i = 1 to |Sy| - 1
      - For j=1 to min{7, |Sy| - i}
        - let p_ij = ith, (i+j)th points of Sy
        - If d(p, q) < best
          - bestpair = (p,q), best = d(p,q)
*** Correctness Claim
    - Claim: let p in Q, q in R be a split pair with d(p, q) < delta
    - Then:
      - A: p and q are members of Sy
      - B: p and q are at most 7 positions apart in Sy
    - Corollary 1: If the closest pair of P is a split pair, then ClosestSplitPair
      finds it
    - Corollary 2: ClosestPair is correct, and runs in O(n log n) time
*** Proof of Correctness Claim (A)
    - let p = (x1, y1) in Q, q=(x2, y2) in R, d(p, q) < delta
      - note: since d(p, q) < delta, |x1 - x2| < delta and |y1 - y2| < delta
    - Proof: [p and q are members of Sy, ie x1, x2 in [x| - delta, x| + delta]]
      - note: p in Q => x1 <= x| and q in R => x2 >= x|
      - => x1, x2 in [x| - delta, x| + delta]
*** Proof of Correctness Claim (B)
    - B: p=(x1, y1) and q=(x2,y2) are at most 7 positions apart in Sy
    - key picture: draw delta/2 * delta/2 boxes with center x| and bottom min {y1,y2}
    - lemma 1: all points of Sy with y-coordinate between those of p and q, inclusive,
      lie in one of these 8 boxes
      - proof: first, recall y-coordinates of p,q differ by < delta
      - second, by definition of Sy, all have x-coordinates between x| - delta and
        x| + delta
    - lemma 2: at most one point of p in each box
      - proof: by contradiction. Suppose a,b lie in the same box. Then:
        - (i) a,b are either both in Q or both in R
        - (ii) d(a, b) <= delta/2 * square root of 2 < delta
        - But (i) and (ii) contradict the definition of delta (as smallest distance between
          pair of points in Q or in R)
*** Final wrap-up
    - lemmas 1 and 2 => at most 8 points in this picture (including p and q)
    - => positions of p,q in Sy differ by at most 7
* Master Method
  - Motivation: potentially useful algorithmic ideas often need mathematical analysis to 
   evaluate
  - Recall: grade-school multiplication algorithm uses theta(n^2) operations to
    multiply two n-digit numbers
** A recursive algorithm
   - recursive approach: write x=10^(n/2)a + b y=10^(n/2)c + d
     [where a,b,c,d are n/2 digit numbers]
   - So: x*y=10^n(ac) + 10^(n/2)(ad+bc) + bd (*)
   - Algorithm #1: recursively compute ac, ad, bc, bd then compute (*) in obvious
     way
     - T(n) = maximum number of operations this algorithm needs to multiply two 
       n-digit numbers
     - recurrence: express T(n) in terms of running time of recursive calls
     - base case: T(1) <= a constant
     - for all n > 1: T(n) <= 4T(n/2) + O(n)
** A better recursive algorithm
   - algorithm #2 (Gauss): recursively compute (1) ac, (2) bd, (3) (a+b)(c+d)
     [recall ad+bc=(3) - (1) - (2)]
   - new recurrence:
     - base case: T(1) <= a constant
     - for all n > 1: T(n) <= 3T(n/2) + O(n)
** The Master Method
   - cool feature: a "black box" for solving recurrences
   - assumption: all subproblems have equal size
*** recurrence format
    - base case: T(n) <= a constant for all sufficiently small n
    - for all larger n:
      - T(n) <= aT(n/b) + O(n^d)
      - where
        - a = number of recursive calls (>= 1)
        - b = input size shrinkage factor (> 1)
        - d = exponent in running time of "combine step" (>=0)
        - [a,b,d independent of n]
*** definition
    - T(n) = 
      - O(n^d log n) if a = b^d
      - O(n^d) if a < b^d
      - O(n^log_b(a)) if a > b^d
*** examples
**** example 1: Merge Sort
     - a = 2
     - b = 2
     - d = 1
     - a = 2 = b^d
     - => T(n) <= O(n^d log n) = O(n log n)
**** example 2: binary search
     - a = 1
     - b = 2
     - d = 0
     - a = b^d => T(n) = O(n^d log n) = O(log n)
**** example 3: integer multiplication
     - a = 4
     - b = 2
     - d = 1
     - b^d=2 < a
     - => T(n) = O(n^log_b(a)) = O(n^log_2(4)) = O(n^2)
**** example 4: gauss's integer multiplication
     - a = 3
     - b = 2
     - d = 1
     - b^d=2 < a
     - => T(n) = O(n^log_b(a)) = O(n^log_2(3))
**** example 5: strassen's matrix multiplication
     - a = 7
     - b = 2
     - d = 1
     - b^d=4 < a
     - => T(n) = O(n^log_2(7)) = O(n^2.81)
     - => beats the naive iterative algorithm O(n^3)
**** example 6: fictitious recurrence
     - T(n) <= 2T(n/2) + O(n^2)
     - a = 2
     - b = 2
     - d = 2
     - b^d=4 > a (case 2)
     - => T(n) = O(n^2)
** Proof
*** Preamble
    - assume: recurrence is (for some constant c)
      - (i) T(1) <= c
      - (ii) T(n) <= aT(n/b) + cn^d 
      - and n is a power of b (general case is similar, but more tedious)
    - Idea: generalize merge sort analysis (ie, use a recursion tree)
*** Work at a single recursion level
    - a^j = # of level-j subproblems
    - n/b^j = size of each level-j subproblem
    - c * (n/b^j)^d = work per level-j sub problem
    - <= a^j * c * (n/b^j)^d = cn^d * (a/b^d)^j
